#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¤– ML DATA COLLECTOR - Recolector de Datos Reales del Sistema
==============================================================

Recolecta datos reales de METACORTEX para entrenar modelos ML:
    pass  # TODO: Implementar
- Interacciones de usuarios
- MÃ©tricas del sistema
- Patrones de cachÃ©
- Consultas y respuestas
- Rendimiento de agentes

Autor: METACORTEX Team
Fecha: 2025-10-17
"""

import json
import logging
import sqlite3
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Optional
import pandas as pd

try:
    import psutil

    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False

try:
    from memory_system import get_memory

    MEMORY_AVAILABLE = True
except ImportError:
    MEMORY_AVAILABLE = False

try:
    from advanced_cache_system import get_global_cache

    CACHE_AVAILABLE = True
except ImportError:
    CACHE_AVAILABLE = False

logger = logging.getLogger(__name__)


class MLDataCollector:
    """
    Recolecta datos reales del sistema para entrenar modelos ML
    """

    def __init__(self, data_dir: str = "ml_data"):
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(exist_ok=True)

        # Subdirectorios
        self.training_dir = self.data_dir / "training"
        self.raw_dir = self.data_dir / "raw"
        self.processed_dir = self.data_dir / "processed"

        for dir_path in [self.training_dir, self.raw_dir, self.processed_dir]:
            dir_path.mkdir(exist_ok=True)

        # Base de datos SQLite para datos raw
        self.db_path = self.raw_dir / "system_data.db"
        self._init_database()

        # Conexiones a sistemas
        self.memory = get_memory() if MEMORY_AVAILABLE else None
        self.cache = get_global_cache() if CACHE_AVAILABLE else None

        logger.info("âœ… ML Data Collector inicializado")
        logger.info(f"   Directorio: {self.data_dir}")

    def _init_database(self):
        """Inicializa base de datos SQLite"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # Tabla: Interacciones de usuarios
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS user_interactions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                user_query TEXT,
                intent TEXT,
                response_time_ms REAL,
                tokens_used INTEGER,
                success BOOLEAN,
                agent_used TEXT,
                context_length INTEGER
            )
        """)

        # Tabla: MÃ©tricas del sistema
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS system_metrics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                cpu_percent REAL,
                memory_percent REAL,
                disk_io_read_mb REAL,
                disk_io_write_mb REAL,
                network_sent_mb REAL,
                network_recv_mb REAL,
                active_processes INTEGER,
                load_category TEXT
            )
        """)

        # Tabla: Patrones de cachÃ©
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS cache_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                cache_key TEXT,
                hit BOOLEAN,
                access_time_ms REAL,
                data_size_kb REAL,
                ttl_seconds INTEGER,
                access_count INTEGER
            )
        """)

        # Tabla: Rendimiento de agentes
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS agent_performance (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                agent_name TEXT,
                task_type TEXT,
                execution_time_ms REAL,
                success BOOLEAN,
                tokens_used INTEGER,
                error_type TEXT
            )
        """)

        conn.commit()
        conn.close()
        logger.info("âœ… Base de datos SQLite inicializada")

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # RECOLECCIÃ“N DE DATOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def collect_user_interaction(
        self,
        user_query: str,
        intent: str,
        response_time_ms: float,
        tokens_used: int,
        success: bool,
        agent_used: str = "unknown",
        context_length: int = 0,
    ):
        """Registra una interacciÃ³n de usuario"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO user_interactions 
            (user_query, intent, response_time_ms, tokens_used, success, agent_used, context_length)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
            (
                user_query,
                intent,
                response_time_ms,
                tokens_used,
                success,
                agent_used,
                context_length,
            ),
        )

        conn.commit()
        conn.close()

    def collect_system_metrics(self):
        """Recolecta mÃ©tricas actuales del sistema"""
        if not PSUTIL_AVAILABLE:
            return

        try:
            # Obtener mÃ©tricas
            cpu = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory().percent
            disk_io = psutil.disk_io_counters()
            net_io = psutil.net_io_counters()

            # Categorizar carga
            avg_load = (cpu + memory) / 2
            if avg_load < 30:
                load_category = "low"
            elif avg_load < 70:
                load_category = "medium"
            else:
                load_category = "high"

            # Guardar
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            cursor.execute(
                """
                INSERT INTO system_metrics 
                (cpu_percent, memory_percent, disk_io_read_mb, disk_io_write_mb,
                 network_sent_mb, network_recv_mb, active_processes, load_category)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    cpu,
                    memory,
                    disk_io.read_bytes / (1024**2),
                    disk_io.write_bytes / (1024**2),
                    net_io.bytes_sent / (1024**2),
                    net_io.bytes_recv / (1024**2),
                    len(psutil.pids()),
                    load_category,
                ),
            )

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Error recolectando mÃ©tricas del sistema: {e}")

    def collect_cache_pattern(
        self,
        cache_key: str,
        hit: bool,
        access_time_ms: float,
        data_size_kb: float = 0,
        ttl_seconds: int = 3600,
        access_count: int = 1,
    ):
        """Registra un patrÃ³n de acceso a cachÃ©"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO cache_patterns 
            (cache_key, hit, access_time_ms, data_size_kb, ttl_seconds, access_count)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (cache_key, hit, access_time_ms, data_size_kb, ttl_seconds, access_count),
        )

        conn.commit()
        conn.close()

    def collect_agent_performance(
        self,
        agent_name: str,
        task_type: str,
        execution_time_ms: float,
        success: bool,
        tokens_used: int = 0,
        error_type: Optional[str] = None,
    ):
        """Registra rendimiento de un agente"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute(
            """
            INSERT INTO agent_performance 
            (agent_name, task_type, execution_time_ms, success, tokens_used, error_type)
            VALUES (?, ?, ?, ?, ?, ?)
        """,
            (
                agent_name,
                task_type,
                execution_time_ms,
                success,
                tokens_used,
                error_type,
            ),
        )

        conn.commit()
        conn.close()

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # GENERACIÃ“N DE DATASETS PARA ENTRENAMIENTO
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def generate_intention_classifier_dataset(
        self, min_samples: int = 100
    ) -> Optional[Path]:
        """
        Genera dataset para clasificador de intenciones

        CaracterÃ­sticas:
        - Longitud de consulta
        - Palabras clave presentes
        - Complejidad sintÃ¡ctica

        Target: intent (coding, search, analysis, chat, etc.)
        """
        conn = sqlite3.connect(self.db_path)

        # Verificar cantidad de datos
        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM user_interactions")
        count = cursor.fetchone()[0]

        if count < min_samples:
            logger.warning(f"âš ï¸ Insuficientes datos: {count}/{min_samples} muestras")
            conn.close()
            return None

        # Extraer datos
        df = pd.read_sql_query(
            """
            SELECT 
                LENGTH(user_query) as query_length,
                (user_query LIKE '%cÃ³digo%' OR user_query LIKE '%code%') as has_coding_keywords,
                (user_query LIKE '%buscar%' OR user_query LIKE '%search%') as has_search_keywords,
                (user_query LIKE '%analizar%' OR user_query LIKE '%analyze%') as has_analysis_keywords,
                context_length,
                tokens_used,
                intent as target
            FROM user_interactions
            WHERE intent IS NOT NULL
        """,
            conn,
        )

        conn.close()

        # Guardar
        output_path = self.training_dir / "intention_classifier.csv"
        df.to_csv(output_path, index=False)

        logger.info(f"âœ… Dataset creado: {output_path} ({len(df)} muestras)")
        return output_path

    def generate_load_predictor_dataset(self, min_samples: int = 100) -> Optional[Path]:
        """
        Genera dataset para predictor de carga del sistema

        CaracterÃ­sticas:
        - CPU actual
        - Memoria actual
        - I/O disco
        - Procesos activos
        - Hora del dÃ­a

        Target: load_category (low, medium, high)
        """
        conn = sqlite3.connect(self.db_path)

        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM system_metrics")
        count = cursor.fetchone()[0]

        if count < min_samples:
            logger.warning(f"âš ï¸ Insuficientes datos: {count}/{min_samples} muestras")
            conn.close()
            return None

        df = pd.read_sql_query(
            """
            SELECT 
                cpu_percent,
                memory_percent,
                disk_io_read_mb,
                disk_io_write_mb,
                active_processes,
                CAST(strftime('%H', timestamp) AS INTEGER) as hour_of_day,
                load_category as target
            FROM system_metrics
            WHERE load_category IS NOT NULL
        """,
            conn,
        )

        conn.close()

        output_path = self.training_dir / "load_predictor.csv"
        df.to_csv(output_path, index=False)

        logger.info(f"âœ… Dataset creado: {output_path} ({len(df)} muestras)")
        return output_path

    def generate_cache_optimizer_dataset(
        self, min_samples: int = 100
    ) -> Optional[Path]:
        """
        Genera dataset para optimizador de cachÃ©

        CaracterÃ­sticas:
        - TamaÃ±o de datos
        - Frecuencia de acceso
        - Tiempo de acceso
        - TTL actual

        Target: hit (1=cache hit, 0=cache miss)
        """
        conn = sqlite3.connect(self.db_path)

        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM cache_patterns")
        count = cursor.fetchone()[0]

        if count < min_samples:
            logger.warning(f"âš ï¸ Insuficientes datos: {count}/{min_samples} muestras")
            conn.close()
            return None

        df = pd.read_sql_query(
            """
            SELECT 
                data_size_kb,
                access_count,
                access_time_ms,
                ttl_seconds,
                CAST(hit AS INTEGER) as target
            FROM cache_patterns
        """,
            conn,
        )

        conn.close()

        output_path = self.training_dir / "cache_optimizer.csv"
        df.to_csv(output_path, index=False)

        logger.info(f"âœ… Dataset creado: {output_path} ({len(df)} muestras)")
        return output_path

    def generate_agent_performance_dataset(
        self, min_samples: int = 100
    ) -> Optional[Path]:
        """
        Genera dataset para predictor de rendimiento de agentes

        CaracterÃ­sticas:
        - Tipo de tarea
        - Tokens usados
        - Agente usado

        Target: execution_time_ms
        """
        conn = sqlite3.connect(self.db_path)

        cursor = conn.cursor()
        cursor.execute("SELECT COUNT(*) FROM agent_performance")
        count = cursor.fetchone()[0]

        if count < min_samples:
            logger.warning(f"âš ï¸ Insuficientes datos: {count}/{min_samples} muestras")
            conn.close()
            return None

        df = pd.read_sql_query(
            """
            SELECT 
                agent_name,
                task_type,
                tokens_used,
                CAST(success AS INTEGER) as success,
                execution_time_ms as target
            FROM agent_performance
        """,
            conn,
        )

        # One-hot encoding para agent_name y task_type
        df = pd.get_dummies(df, columns=["agent_name", "task_type"])

        conn.close()

        output_path = self.training_dir / "agent_performance.csv"
        df.to_csv(output_path, index=False)

        logger.info(f"âœ… Dataset creado: {output_path} ({len(df)} muestras)")
        return output_path

    def generate_all_datasets(
        self, min_samples: int = 100
    ) -> Dict[str, Optional[Path]]:
        """Genera todos los datasets disponibles"""
        logger.info("ğŸ“Š Generando todos los datasets...")

        datasets = {
            "intention_classifier": self.generate_intention_classifier_dataset(
                min_samples
            ),
            "load_predictor": self.generate_load_predictor_dataset(min_samples),
            "cache_optimizer": self.generate_cache_optimizer_dataset(min_samples),
            "agent_performance": self.generate_agent_performance_dataset(min_samples),
        }

        # Resumen
        created = sum(1 for path in datasets.values() if path is not None)
        logger.info(f"âœ… Datasets generados: {created}/{len(datasets)}")

        return datasets

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # UTILIDADES
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def get_data_stats(self) -> Dict[str, int]:
        """Obtiene estadÃ­sticas de datos recolectados"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        stats = {}

        for table in [
            "user_interactions",
            "system_metrics",
            "cache_patterns",
            "agent_performance",
        ]:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            stats[table] = cursor.fetchone()[0]

        conn.close()

        return stats

    def clear_old_data(self, days: int = 30):
        """Elimina datos antiguos"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cutoff = datetime.now() - timedelta(days=days)

        for table in [
            "user_interactions",
            "system_metrics",
            "cache_patterns",
            "agent_performance",
        ]:
            cursor.execute(
                f"DELETE FROM {table} WHERE timestamp < ?", (cutoff.isoformat(),)
            )

        deleted = cursor.rowcount
        conn.commit()
        conn.close()

        logger.info(f"ğŸ—‘ï¸ Eliminados {deleted} registros antiguos (>{days} dÃ­as)")

    def export_to_json(self, output_path: Optional[Path] = None) -> Path:
        """Exporta todos los datos a JSON"""
        if output_path is None:
            output_path = (
                self.raw_dir / f"export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )

        conn = sqlite3.connect(self.db_path)

        data = {}
        for table in [
            "user_interactions",
            "system_metrics",
            "cache_patterns",
            "agent_performance",
        ]:
            df = pd.read_sql_query(f"SELECT * FROM {table}", conn)
            data[table] = df.to_dict(orient="records")

        conn.close()

        with open(output_path, "w") as f:
            json.dump(data, f, indent=2, default=str)

        logger.info(f"ğŸ“¦ Datos exportados: {output_path}")
        return output_path


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

_global_data_collector = None


def get_data_collector(**kwargs) -> MLDataCollector:
    """Obtiene instancia global del data collector"""
    global _global_data_collector
    if _global_data_collector is None:
        _global_data_collector = MLDataCollector(**kwargs)
    return _global_data_collector


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TESTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)

    print("ğŸ¤– ML Data Collector - Test")
    print("=" * 60)

    collector = get_data_collector()

    # Generar datos de ejemplo
    print("\nğŸ“Š Generando datos de ejemplo...")

    # Interacciones de usuario
    intents = ["coding", "search", "analysis", "chat", "debug"]
    queries = [
        "Crea una funciÃ³n para calcular fibonacci",
        "Busca informaciÃ³n sobre Python",
        "Analiza el rendimiento de este cÃ³digo",
        "Hola, Â¿cÃ³mo estÃ¡s?",
        "Este cÃ³digo tiene un error",
    ]

    for i in range(50):
        intent = intents[i % len(intents)]
        query = queries[i % len(queries)]
        collector.collect_user_interaction(
            user_query=query,
            intent=intent,
            response_time_ms=100 + (i * 10),
            tokens_used=50 + i,
            success=True,
            agent_used="test_agent",
            context_length=200,
        )

    # MÃ©tricas del sistema
    for _ in range(50):
        collector.collect_system_metrics()

    # Patrones de cachÃ©
    for i in range(50):
        collector.collect_cache_pattern(
            cache_key=f"key_{i}",
            hit=i % 2 == 0,
            access_time_ms=5 + i,
            data_size_kb=10 + i,
            ttl_seconds=3600,
            access_count=i,
        )

    # Rendimiento de agentes
    agents = ["programming_agent", "search_agent", "analysis_agent"]
    tasks = ["code_generation", "web_search", "data_analysis"]

    for i in range(50):
        collector.collect_agent_performance(
            agent_name=agents[i % len(agents)],
            task_type=tasks[i % len(tasks)],
            execution_time_ms=100 + (i * 5),
            success=True,
            tokens_used=50 + i,
        )

    print("âœ… Datos de ejemplo generados")

    # EstadÃ­sticas
    print("\nğŸ“Š EstadÃ­sticas:")
    stats = collector.get_data_stats()
    for table, count in stats.items():
        print(f"   {table}: {count} registros")

    # Generar datasets
    print("\nğŸ“Š Generando datasets para entrenamiento...")
    datasets = collector.generate_all_datasets(min_samples=10)

    for name, path in datasets.items():
        if path:
            print(f"   âœ… {name}: {path}")
        else:
            print(f"   âš ï¸ {name}: Insuficientes datos")

    print("\nâœ… Test completado")