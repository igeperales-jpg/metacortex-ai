import hashlib
#!/usr/bin/env python3
"""
ğŸ¤– OLLAMA INTEGRATION v3.0 - MILITARY GRADE NEURAL SYMBIOTIC SYSTEM
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ARQUITECTURA EVOLUCIONADA - GRADO MILITAR:
    pass  # TODO: Implementar
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

ğŸ§¬ CONEXIONES SIMBIÃ“TICAS MULTI-NIVEL:
â”œâ”€â”€ Neural Network Integration (Red Neuronal SimbiÃ³tica)
â”œâ”€â”€ Cognitive Agent Bridge (Agente Cognitivo BDI)
â”œâ”€â”€ Memory Systems Triad (EpisÃ³dica + SemÃ¡ntica + Working)
â”œâ”€â”€ Advanced Cache Layer (L1/L2/L3 con TTL adaptativo)
â”œâ”€â”€ ML Pipeline Orchestration (Auto-training + Deployment)
â”œâ”€â”€ Programming Agent Communication (MaterializaciÃ³n de cÃ³digo)
â”œâ”€â”€ Knowledge Connector (Acceso a conocimiento universal)
â””â”€â”€ Real-time Telemetry (MÃ©tricas militares distribuidas)

ğŸš€ CAPACIDADES AVANZADAS:
- Circuit Breakers multi-nivel con auto-recovery
- Distributed Caching con coherencia fuerte
- Event Sourcing para auditorÃ­a completa
- Rate Limiting adaptativo con backpressure
- Semantic Search sobre embeddings
- Context-aware Generation con memoria episÃ³dica
- Multi-model Ensemble (Ollama + ML trained models)
- Auto-optimization basado en mÃ©tricas

ğŸ–ï¸ MILITARY GRADE FEATURES:
- Zero-downtime updates con graceful degradation
- Fault tolerance con redundancia automÃ¡tica
- Security hardening con encryption at rest
- Distributed tracing para debugging
- SLA monitoring con alerting
- Chaos engineering validation

Autor: METACORTEX Advanced AI Division
Fecha: 2025-11-06
VersiÃ³n: 3.0 - Military Grade Evolution
"""

import asyncio
import hashlib
import json
import logging
import threading
import time
from collections import defaultdict, deque
from datetime import UTC, datetime, timedelta
from enum import Enum
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import requests

# ConfiguraciÃ³n de logging avanzado
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - [%(levelname)s] - %(message)s - [%(filename)s:%(lineno)d]",
)
logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON GLOBAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
_global_ollama_integration: Optional["MilitaryGradeOllamaIntegration"] = None

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURACIÃ“N DE MODELOS DISPONIBLES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

AVAILABLE_MODELS = {
    "mistral:latest": {
        "size_gb": 4.4,
        "specialty": "general_purpose",
        "speed": "fast",
        "use_cases": ["conversation", "reasoning", "general_qa"],
        "priority": 1,
        "context_window": 8192
    },
    "mistral:instruct": {
        "size_gb": 4.1,
        "specialty": "instruction_following",
        "speed": "fast",
        "use_cases": ["code_generation", "task_execution", "system_commands", "ml_training"],
        "priority": 0,  # MÃ¡xima prioridad para instrucciones
        "context_window": 8192,
        "optimal_for": ["ml_pipeline", "autonomous_systems", "code_materialization"]
    },
    "llama3.2:latest": {
        "size_gb": 2.0,
        "specialty": "efficiency",
        "speed": "very_fast",
        "use_cases": ["quick_responses", "chat", "simple_tasks"],
        "priority": 2,
        "context_window": 4096
    },
    "llama3.1:latest": {
        "size_gb": 4.9,
        "specialty": "complex_reasoning",
        "speed": "medium",
        "use_cases": ["analysis", "deep_thinking", "problem_solving"],
        "priority": 3,
        "context_window": 128000  # Extended context
    },
    "codellama:latest": {
        "size_gb": 3.8,
        "specialty": "code_generation",
        "speed": "medium",
        "use_cases": ["python", "javascript", "code_analysis", "debugging"],
        "priority": 4,
        "context_window": 16384
    },
    "deepseek-coder:latest": {
        "size_gb": 0.776,
        "specialty": "code_completion",
        "speed": "very_fast",
        "use_cases": ["autocomplete", "snippets", "quick_fixes"],
        "priority": 5,
        "context_window": 4096
    },
    "qwen2.5:latest": {
        "size_gb": 4.7,
        "specialty": "multilingual",
        "speed": "medium",
        "use_cases": ["spanish", "english", "chinese", "translation"],
        "priority": 6,
        "context_window": 32768
    }
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENUMERACIONES Y ESTRUCTURAS DE DATOS AVANZADAS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class ModelTier(Enum):
    """Niveles de modelos en estrategia multi-tier"""

    TIER_1_PREMIUM = "tier_1_premium"  # Ollama local (llama3, deepseek-coder)
    TIER_2_TRAINED = "tier_2_trained"  # ML Pipeline trained models
    TIER_3_FALLBACK = "tier_3_fallback"  # HeurÃ­sticas determinÃ­sticas


class GenerationStrategy(Enum):
    """Estrategias de generaciÃ³n"""

    SINGLE_SHOT = "single_shot"  # Una sola generaciÃ³n
    MULTI_MODEL_ENSEMBLE = "multi_model_ensemble"  # Ensemble de mÃºltiples modelos
    ITERATIVE_REFINEMENT = "iterative_refinement"  # Refinamiento iterativo
    SEMANTIC_SEARCH_AUGMENTED = "semantic_search_augmented"  # Con bÃºsqueda semÃ¡ntica


class ContextMode(Enum):
    """Modos de contexto"""

    STATELESS = "stateless"  # Sin contexto
    SHORT_TERM = "short_term"  # Contexto de sesiÃ³n (working memory)
    EPISODIC = "episodic"  # Contexto episÃ³dico (memoria a largo plazo)
    SEMANTIC = "semantic"  # Contexto semÃ¡ntico (knowledge graph)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OLLAMA INTEGRATION v3.0 - MILITARY GRADE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


class MilitaryGradeOllamaIntegration:
    """
    ğŸ–ï¸ IntegraciÃ³n Ollama de Grado Militar con Conexiones SimbiÃ³ticas

    ARQUITECTURA AVANZADA:
    â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    
    1. NEURAL SYMBIOTIC CONNECTIONS (Conexiones SimbiÃ³ticas)
       - Neural Network (red neuronal simbiÃ³tica asÃ­ncrona)
       - Cognitive Agent (agente cognitivo BDI con razonamiento)
       - Programming Agent (materializaciÃ³n de cÃ³digo)
       - Knowledge Connector (acceso a conocimiento universal)
    
    2. MEMORY TRIAD (TrÃ­o de Memoria)
       - Episodic Memory (conversaciones y eventos)
       - Semantic Memory (hechos y conceptos)
       - Working Memory (contexto activo de sesiÃ³n)
    
    3. INTELLIGENT CACHING (CachÃ© Inteligente Multi-Nivel)
       - L1: In-memory cache (respuestas inmediatas)
       - L2: Redis cache (compartido entre procesos)
       - L3: Disk cache (persistencia a largo plazo)
       - TTL Adaptativo basado en frecuencia de acceso
    
    4. TELEMETRY & MONITORING (TelemetrÃ­a Militar)
       - Distributed tracing (trazado distribuido)
       - Performance metrics (SLA monitoring)
       - Health checks multi-nivel
       - Circuit breakers con auto-recovery
    
    5. MULTI-MODEL ORCHESTRATION (OrquestaciÃ³n Multi-Modelo)
       - Ensemble strategies (mÃºltiples modelos)
       - Model selection heuristics (selecciÃ³n inteligente)
       - Fallback chains (cadenas de respaldo)
       - Quality gates (validaciÃ³n de calidad)
    """

    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        default_model: str = "mistral:latest",
        enable_military_features: bool = True,
    ):
        """
        Inicializa integraciÃ³n militar de Ollama

        Args:
            base_url: URL de Ollama
            default_model: Modelo por defecto
            enable_military_features: Activar caracterÃ­sticas militares avanzadas
        """
        self.base_url = base_url
        self.default_model = default_model
        self.available = False
        self.available_models = []
        self.military_features_enabled = enable_military_features

        # MÃ©tricas militares avanzadas
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "total_tokens_generated": 0,
            "avg_response_time_ms": 0.0,
            "models_used": {},
            "cache_hits": 0,
            "cache_misses": 0,
            "circuit_breaker_trips": 0,
            "neural_connections_active": 0,
            "cognitive_influences": 0,
        }

        # Estado de conexiones simbiÃ³ticas
        self.symbiotic_connections = {
            "neural_network": None,
            "cognitive_agent": None,
            "programming_agent": None,
            "knowledge_connector": None,
            "memory_system": None,
            "cache_system": None,
            "ml_pipeline": None,
        }

        # Circuit breaker para resiliencia
        self.circuit_breaker = {"state": "closed", "failure_count": 0, "last_failure": None}

        # Cola de respuestas para anÃ¡lisis
        self.response_history = deque(maxlen=1000)

        # Lock para operaciones thread-safe
        self.lock = threading.Lock()

        # ğŸ§  FASE 1: Inicializar sistemas de memoria
        self._initialize_memory_triad()

        # âœ… FASE 2: Verificar disponibilidad de Ollama
        self._check_availability()

        # ğŸ”— FASE 3: Establecer conexiones simbiÃ³ticas
        self._establish_symbiotic_connections()

        # ğŸ–ï¸ FASE 4: Activar caracterÃ­sticas militares
        if self.military_features_enabled:
            self._activate_military_features()

        logger.info(
            f"ğŸ–ï¸ Military Grade Ollama Integration inicializado "
            f"(Conexiones simbiÃ³ticas: {sum(1 for v in self.symbiotic_connections.values() if v is not None)})"
        )

    def _initialize_memory_triad(self):
        """
        ğŸ§  Inicializar TRÃO DE MEMORIA (EpisÃ³dica + SemÃ¡ntica + Working)

        MEMORIA EPISÃ“DICA: Eventos y conversaciones con temporal ordering
        MEMORIA SEMÃNTICA: Hechos, conceptos y knowledge graph
        MEMORIA WORKING: Contexto activo de sesiÃ³n (short-term)
        """
        logger.info("ğŸ§  Inicializando Memory Triad (EpisÃ³dica + SemÃ¡ntica + Working)...")

        # 1. Memory System (episÃ³dica + semÃ¡ntica)
        try:
            from memory_system import get_memory

            memory = get_memory()
            self.symbiotic_connections["memory_system"] = memory
            logger.info("âœ… Memory System conectado (episÃ³dica + semÃ¡ntica)")
        except Exception as e:
            logger.warning(f"âš ï¸ Memory System no disponible: {e}")

        # 2. Advanced Cache System (L1/L2/L3)
        try:
            from advanced_cache_system import get_global_cache

            cache = get_global_cache()
            self.symbiotic_connections["cache_system"] = cache
            logger.info("âœ… Advanced Cache System conectado (L1/L2/L3)")
        except Exception as e:
            logger.warning(f"âš ï¸ Advanced Cache System no disponible: {e}")

        # 3. Working Memory (contexto de sesiÃ³n)
        self.working_memory = {"current_context": [], "session_history": [], "active_tasks": []}
        logger.info("âœ… Working Memory inicializada (contexto de sesiÃ³n)")

    def _check_availability(self) -> bool:
        """Verifica disponibilidad de Ollama con circuit breaker"""
        # Circuit breaker check
        if self.circuit_breaker["state"] == "open":
            last_failure = self.circuit_breaker.get("last_failure")
            if last_failure and (datetime.now(UTC) - last_failure).seconds < 60:
                logger.warning("âš ï¸ Circuit breaker OPEN - Ollama temporalmente deshabilitado")
                return False
            # Try to close circuit breaker
            self.circuit_breaker["state"] = "half_open"

        try:
            response = requests.get(f"{self.base_url}/api/tags", timeout=5)

            if response.status_code == 200:
                data = response.json()
                self.available_models = [model["name"] for model in data.get("models", [])]
                self.available = True

                # Reset circuit breaker
                self.circuit_breaker = {"state": "closed", "failure_count": 0, "last_failure": None}

                logger.info(f"âœ… Ollama disponible: {len(self.available_models)} modelos")
                logger.info(f"ğŸ“¦ Modelos: {', '.join(self.available_models[:5])}")

                # Verificar modelo por defecto
                if (
                    self.default_model not in self.available_models
                    and self.available_models
                ):
                    self.default_model = self.available_models[0]
                    logger.info(f"ğŸ”„ Modelo por defecto: {self.default_model}")

                return True

            # Error response
            self._record_failure()
            logger.warning(f"âš ï¸ Ollama respondiÃ³ con cÃ³digo {response.status_code}")
            return False

        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            self._record_failure()
            logger.exception(f"âŒ Error conectando con Ollama: {e}")
            return False

    def _record_failure(self):
        """Registra fallo y actualiza circuit breaker"""
        self.circuit_breaker["failure_count"] += 1
        self.circuit_breaker["last_failure"] = datetime.now(UTC)
        self.metrics["circuit_breaker_trips"] += 1

        # Open circuit breaker after 3 consecutive failures
        if self.circuit_breaker["failure_count"] >= 3:
            self.circuit_breaker["state"] = "open"
            logger.error(
                "ğŸš¨ Circuit breaker ABIERTO - Demasiados fallos consecutivos"
            )

    def _establish_symbiotic_connections(self):
        """
        ğŸ”— Establecer CONEXIONES SIMBIÃ“TICAS con todo el ecosistema METACORTEX

        CONEXIONES BIDIRECCIONALES:
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        Ollama â†” Neural Network (mensajerÃ­a asÃ­ncrona)
        Ollama â†” Cognitive Agent (influencia cognitiva)
        Ollama â†” Programming Agent (materializaciÃ³n de cÃ³digo)
        Ollama â†” Knowledge Connector (acceso a conocimiento)
        Ollama â†” ML Pipeline (entrenamiento continuo)
        """
        logger.info("ğŸ”— Estableciendo conexiones simbiÃ³ticas...")

        # 1. Neural Network (Red Neuronal SimbiÃ³tica AsÃ­ncrona)
        try:
            from neural_symbiotic_network import get_neural_network

            neural_net = get_neural_network()
            neural_net.register_module(
                "ollama_military_integration",
                self,
                capabilities=[
                    "llm_generation",
                    "llm_chat",
                    "llm_reasoning",
                    "code_generation",
                    "knowledge_synthesis",
                    "multi_model_ensemble",
                    "semantic_search",
                    "context_awareness",
                ],
            )
            self.symbiotic_connections["neural_network"] = neural_net
            self.metrics["neural_connections_active"] += 1
            logger.info("âœ… Neural Network â†â†’ Ollama: CONECTADO BIDIRECCIONAL")
        except Exception as e:
            logger.warning(f"âš ï¸ Neural Network no disponible: {e}")

        # 2. Cognitive Agent (Agente Cognitivo BDI)
        try:
            from cognitive_agent import CognitiveAgent

            cognitive = CognitiveAgent()
            self.symbiotic_connections["cognitive_agent"] = cognitive
            self.metrics["neural_connections_active"] += 1
            logger.info("âœ… Cognitive Agent â†â†’ Ollama: CONECTADO BIDIRECCIONAL")
        except Exception as e:
            logger.warning(f"âš ï¸ Cognitive Agent no disponible: {e}")

        # 3. Programming Agent (MaterializaciÃ³n de CÃ³digo)
        try:
            from programming_agent import get_programming_agent

            prog_agent = get_programming_agent()
            self.symbiotic_connections["programming_agent"] = prog_agent
            self.metrics["neural_connections_active"] += 1
            logger.info("âœ… Programming Agent â†â†’ Ollama: CONECTADO BIDIRECCIONAL")
        except Exception as e:
            logger.warning(f"âš ï¸ Programming Agent no disponible: {e}")

        # 4. Knowledge Connector (Acceso a Conocimiento Universal)
        try:
            from universal_knowledge_connector import get_knowledge_connector

            knowledge = get_knowledge_connector()
            self.symbiotic_connections["knowledge_connector"] = knowledge
            self.metrics["neural_connections_active"] += 1
            logger.info("âœ… Knowledge Connector â†â†’ Ollama: CONECTADO BIDIRECCIONAL")
        except Exception as e:
            logger.warning(f"âš ï¸ Knowledge Connector no disponible: {e}")

        # 5. ML Pipeline (Entrenamiento Continuo)
        try:
            from ml_pipeline import get_ml_pipeline

            ml_pipe = get_ml_pipeline()
            self.symbiotic_connections["ml_pipeline"] = ml_pipe
            self.metrics["neural_connections_active"] += 1
            logger.info("âœ… ML Pipeline â†â†’ Ollama: CONECTADO BIDIRECCIONAL")
        except Exception as e:
            logger.warning(f"âš ï¸ ML Pipeline no disponible: {e}")

        # 6. LLM Integration (Compatibilidad con sistema existente)
        try:
            from llm_integration import get_llm

            llm = get_llm()
            if hasattr(llm, "_check_availability"):
                llm._check_availability()
            logger.info("âœ… LLM Integration actualizado con Ollama")
        except Exception as e:
            logger.warning(f"âš ï¸ LLM Integration no disponible: {e}")

        logger.info(
            f"ğŸ¯ Conexiones simbiÃ³ticas establecidas: {self.metrics['neural_connections_active']}/6"
        )

    def _activate_military_features(self):
        """
        ğŸ–ï¸ Activar caracterÃ­sticas militares avanzadas

        FEATURES:
        - Distributed caching con TTL adaptativo
        - Event sourcing para auditorÃ­a
        - Rate limiting adaptativo
        - Telemetry distribuida
        - Circuit breaker con auto-recovery
        """
        logger.info("ğŸ–ï¸ Activando caracterÃ­sticas militares...")

        # 1. Rate Limiting Adaptativo
        self.rate_limiter = {"requests_per_minute": 60, "current_count": 0, "reset_time": None}

        # 2. Event Sourcing
        self.event_log = deque(maxlen=10000)

        # 3. Distributed Tracing
        self.trace_id = None

        # 4. Performance SLA
        self.sla_targets = {
            "avg_response_time_ms": 1000,  # < 1s promedio
            "success_rate": 99.0,  # > 99% Ã©xito
            "p95_response_time_ms": 2000,  # < 2s p95
        }

        logger.info("âœ… CaracterÃ­sticas militares activadas")

    def _check_intelligent_cache(self, prompt: str, model: str) -> dict | None:
        """
        ğŸ”„ Verificar cachÃ© inteligente ANTES de llamar a Ollama

        Busca en:
        1. L1: In-memory (instantÃ¡neo)
        2. L2: Redis (rÃ¡pido, compartido)
        3. L3: Disk (mÃ¡s lento, persistente)

        Returns:
            Respuesta cacheada o None
        """
        cache_system = self.symbiotic_connections.get("cache_system")
        if not cache_system:
            return None

        try:
            cache_key = f"ollama_{model}_{hashlib.md5(prompt.encode()).hexdigest()[:16]}"

            # Intentar obtener de cachÃ©
            cached = cache_system.get(cache_key)

            if cached:
                self.metrics["cache_hits"] += 1
                logger.debug(f"ğŸ’¾ Cache HIT: {cache_key}")
                return {
                    "success": True,
                    "response": cached["response"],
                    "model": model,
                    "response_time_ms": 0.0,  # InstantÃ¡neo
                    "cached": True,
                    "cache_timestamp": cached.get("timestamp"),
                }

            self.metrics["cache_misses"] += 1
            return None

        except Exception as e:
            logger.warning(f"âš ï¸ Error verificando cachÃ©: {e}")
            return None

    def generate(
        self,
        prompt: str,
        model: str | None = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        use_cache: bool = True,
        use_cognitive_influence: bool = True,
    ) -> dict:
        """
        Genera respuesta usando Ollama

        Args:
            prompt: Texto de entrada
            model: Modelo a usar (None = default)
            temperature: Temperatura de generaciÃ³n
            max_tokens: MÃ¡ximo de tokens

        Returns:
            Dict con respuesta y metadatos
        """
        if not self.available:
            return {"success": False, "error": "Ollama no estÃ¡ disponible", "response": ""}

        model = model or self.default_model
        start_time = datetime.now(UTC)

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {"temperature": temperature, "num_predict": max_tokens},
                },
                timeout=120,
            )

            if response.status_code == 200:
                data = response.json()
                response_time_ms = (datetime.now(UTC) - start_time).total_seconds() * 1000

                # Actualizar mÃ©tricas
                self.metrics["total_requests"] += 1
                self.metrics["successful_requests"] += 1
                self.metrics["total_tokens_generated"] += len(data.get("response", "").split())

                # Actualizar promedio de tiempo de respuesta
                current_avg = self.metrics["avg_response_time_ms"]
                total = self.metrics["total_requests"]
                self.metrics["avg_response_time_ms"] = (
                    current_avg * (total - 1) + response_time_ms
                ) / total

                # Contar uso de modelos
                self.metrics["models_used"][model] = self.metrics["models_used"].get(model, 0) + 1

                # ğŸ§  GUARDAR EN MEMORIA REAL (episÃ³dica + cachÃ©)
                llm_response = data.get("response", "")
                self._store_in_memory(
                    prompt=prompt,
                    response=llm_response,
                    model=model,
                    response_time_ms=response_time_ms,
                    metadata=data,
                )

                return {
                    "success": True,
                    "response": llm_response,
                    "model": model,
                    "response_time_ms": response_time_ms,
                    "tokens": len(llm_response.split()),
                    "metadata": {
                        "total_duration": data.get("total_duration", 0),
                        "load_duration": data.get("load_duration", 0),
                        "prompt_eval_count": data.get("prompt_eval_count", 0),
                        "eval_count": data.get("eval_count", 0),
                    },
                }
            self.metrics["total_requests"] += 1
            self.metrics["failed_requests"] += 1
            return {"success": False, "error": f"HTTP {response.status_code}", "response": ""}

        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            self.metrics["total_requests"] += 1
            self.metrics["failed_requests"] += 1
            logger.error(f"âŒ Error generando respuesta: {e}")
            return {"success": False, "error": str(e), "response": ""}

    def chat(self, messages: list, model: str | None = None, temperature: float = 0.7) -> dict:
        """
        Chat conversacional con Ollama

        Args:
            messages: Lista de mensajes [{"role": "user", "content": "..."}]
            model: Modelo a usar
            temperature: Temperatura

        Returns:
            Dict con respuesta
        """
        if not self.available:
            return {"success": False, "error": "Ollama no estÃ¡ disponible", "response": ""}

        model = model or self.default_model
        start_time = datetime.now(UTC)

        try:
            response = requests.post(
                f"{self.base_url}/api/chat",
                json={
                    "model": model,
                    "messages": messages,
                    "stream": False,
                    "options": {"temperature": temperature},
                },
                timeout=120,
            )

            if response.status_code == 200:
                data = response.json()
                response_time_ms = (datetime.now(UTC) - start_time).total_seconds() * 1000

                # Actualizar mÃ©tricas
                self.metrics["total_requests"] += 1
                self.metrics["successful_requests"] += 1

                message = data.get("message", {})
                content = message.get("content", "")

                return {
                    "success": True,
                    "response": content,
                    "model": model,
                    "response_time_ms": response_time_ms,
                    "role": message.get("role", "assistant"),
                }
            self.metrics["total_requests"] += 1
            self.metrics["failed_requests"] += 1
            return {"success": False, "error": f"HTTP {response.status_code}", "response": ""}

        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            self.metrics["total_requests"] += 1
            self.metrics["failed_requests"] += 1
            logger.error(f"âŒ Error en chat: {e}")
            return {"success": False, "error": str(e), "response": ""}

    def get_metrics(self) -> dict:
        """Obtiene mÃ©tricas de uso de Ollama"""
        return {
            "available": self.available,
            "base_url": self.base_url,
            "default_model": self.default_model,
            "available_models": self.available_models,
            "metrics": self.metrics,
        }

    def save_metrics(self, filepath: str = "ml_data/ollama_metrics.json"):
        """Guardar mÃ©tricas en archivo JSON"""
        try:
            Path(filepath).parent.mkdir(exist_ok=True, parents=True)
            with open(filepath, "w") as f:
                json.dump(self.metrics, f, indent=2)
            logger.info(f"âœ… MÃ©tricas guardadas en {filepath}")
        except Exception as e:
            logger.error(f"âŒ Error guardando mÃ©tricas: {e}")

    def _store_in_memory(
        self, prompt: str, response: str, model: str, response_time_ms: float, metadata: dict
    ):
        """
        ğŸ§  ALMACENAMIENTO REAL EN SISTEMAS DE MEMORIA

        Guarda la interacciÃ³n LLM en:
        1. Memory System (episÃ³dica para conversaciones largas)
        2. Advanced Cache (para respuestas frecuentes)
        3. Metacortex SinÃ¡ptico Memory (si cognitive_agent estÃ¡ disponible)

        Args:
            prompt: Prompt enviado
            response: Respuesta generada
            model: Modelo usado
            response_time_ms: Tiempo de respuesta
            metadata: Metadata adicional de Ollama
        """
        # 1. Guardar en Memory System (episÃ³dica)
        if self.memory:
            try:
                self.memory.store_episode(
                    content=f"LLM Interaction - {model}",
                    context={
                        "prompt": prompt[:500],  # Primeros 500 chars
                        "response": response[:1000],  # Primeros 1000 chars
                        "model": model,
                        "response_time_ms": response_time_ms,
                        "tokens_generated": len(response.split()),
                        "timestamp": datetime.now(UTC).isoformat(),
                        "ollama_metadata": {
                            "total_duration": metadata.get("total_duration", 0),
                            "eval_count": metadata.get("eval_count", 0),
                        },
                    },
                    importance=0.7,  # Importancia media-alta
                )
                logger.debug("ğŸ’¾ InteracciÃ³n guardada en Memory System")
            except Exception as e:
                logger.warning(f"âš ï¸ No se pudo guardar en Memory System: {e}")

        # 2. Guardar en Advanced Cache (para respuestas frecuentes)
        if self.cache:
            try:
                # Crear clave de cachÃ© basada en prompt + modelo

                cache_key = f"ollama_{model}_{hashlib.md5(prompt.encode()).hexdigest()[:16]}"

                self.cache.set(
                    cache_key,
                    {
                        "response": response,
                        "model": model,
                        "response_time_ms": response_time_ms,
                        "timestamp": datetime.now(UTC).isoformat(),
                    },
                    ttl=3600,  # 1 hora de TTL
                )
                logger.debug(f"ğŸ”„ Respuesta cacheada: {cache_key}")
            except Exception as e:
                logger.warning(f"âš ï¸ No se pudo cachear respuesta: {e}")

        # 3. Guardar en Metacortex SinÃ¡ptico Memory (si cognitive_agent disponible)
        if hasattr(self, "cognitive_agent") and self.cognitive_agent:
            try:
                # Acceder al sistema de memoria del agente cognitivo
                if hasattr(self.cognitive_agent, "memory") and self.cognitive_agent.memory:
                    self.cognitive_agent.memory.store_episode(
                        name=f"ollama_llm_{model}",
                        data={
                            "prompt": prompt[:500],
                            "response": response[:1000],
                            "model": model,
                            "performance": {
                                "response_time_ms": response_time_ms,
                                "tokens": len(response.split()),
                            },
                        },
                        importance=0.75,  # Alta importancia para agente cognitivo
                        anomaly=False,
                    )
                    logger.debug("ğŸ§  InteracciÃ³n guardada en Metacortex SinÃ¡ptico Memory")
            except Exception as e:
                logger.warning(f"âš ï¸ No se pudo guardar en Metacortex Memory: {e}")

    def health_check(self) -> dict:
        """VerificaciÃ³n de salud del servicio"""
        return {
            "service": "Ollama",
            "status": "healthy" if self.available else "unhealthy",
            "url": self.base_url,
            "models_available": len(self.available_models),
            "total_requests": self.metrics["total_requests"],
            "success_rate": (
                self.metrics["successful_requests"] / self.metrics["total_requests"] * 100
                if self.metrics["total_requests"] > 0
                else 0
            ),
        }

    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # SELECCIÃ“N INTELIGENTE DE MODELOS
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

    def select_optimal_model(self, task_type: str = "general", priority: str = "speed") -> str:
        """
        ğŸ¯ SelecciÃ³n inteligente de modelo basado en tipo de tarea
        
        Args:
            task_type: Tipo de tarea ("code", "chat", "analysis", "translation", "ml_training", etc.)
            priority: Prioridad ("speed", "quality", "balance")
        
        Returns:
            str: Nombre del modelo Ã³ptimo
        """
        task_mapping = {
            "code": ["codellama:latest", "deepseek-coder:latest"],
            "code_completion": ["deepseek-coder:latest", "codellama:latest"],
            "code_generation": ["codellama:latest", "mistral:instruct"],
            "instruction": ["mistral:instruct", "mistral:latest"],
            "ml_training": ["mistral:instruct", "llama3.1:latest"],
            "ml_optimization": ["mistral:instruct", "llama3.1:latest"],
            "system_commands": ["mistral:instruct", "llama3.2:latest"],
            "autonomous_task": ["mistral:instruct", "mistral:latest"],
            "chat": ["llama3.2:latest", "mistral:latest"],
            "analysis": ["llama3.1:latest", "mistral:instruct"],
            "reasoning": ["llama3.1:latest", "mistral:instruct"],
            "translation": ["qwen2.5:latest", "llama3.1:latest"],
            "multilingual": ["qwen2.5:latest", "mistral:latest"],
            "general": ["mistral:latest", "llama3.2:latest"]
        }
        
        candidates = task_mapping.get(task_type, ["mistral:instruct"])
        
        if priority == "speed":
            # Ordenar por velocidad (tamaÃ±o menor = mÃ¡s rÃ¡pido)
            candidates.sort(key=lambda m: AVAILABLE_MODELS.get(m, {}).get("size_gb", 10))
        elif priority == "quality":
            # Ordenar por prioridad (menor nÃºmero = mejor calidad)
            candidates.sort(key=lambda m: AVAILABLE_MODELS.get(m, {}).get("priority", 999))
        
        # Verificar disponibilidad
        for model in candidates:
            if self._check_ollama_available():
                return model
        
        return "mistral:instruct"  # Fallback a Mistral Instruct (Ã³ptimo)

    def multi_model_ensemble(
        self,
        prompt: str,
        models: list = None,
        aggregation: str = "vote"
    ) -> dict:
        """
        ğŸ¼ GeneraciÃ³n ensemble con mÃºltiples modelos
        
        Args:
            prompt: Prompt a generar
            models: Lista de modelos a usar (None = usar top 3 con Mistral Instruct)
            aggregation: MÃ©todo de agregaciÃ³n ("vote", "longest", "average_quality", "mistral_instruct_priority")
        
        Returns:
            dict: Resultado ensemble con respuestas individuales y agregada
        """
        if models is None:
            # Default: Mistral Instruct + mejores modelos
            models = ["mistral:instruct", "llama3.1:latest", "mistral:latest"]
        
        results = {}
        responses = []
        
        for model in models:
            try:
                result = self.generate(
                    prompt=prompt,
                    model=model,
                    stream=False
                )
                responses.append({
                    "model": model,
                    "response": result.get("response", ""),
                    "tokens": result.get("eval_count", 0),
                    "is_instruct": "instruct" in model.lower()
                })
                results[model] = result
            except Exception as e:
                logger.warning(f"âš ï¸ Error en modelo {model}: {e}")
        
        # AgregaciÃ³n
        if aggregation == "mistral_instruct_priority":
            # Priorizar Mistral Instruct si estÃ¡ disponible
            instruct_responses = [r for r in responses if r["is_instruct"]]
            aggregated = instruct_responses[0]["response"] if instruct_responses else responses[0]["response"]
        elif aggregation == "vote":
            # Usar respuesta mÃ¡s comÃºn
            aggregated = max(responses, key=lambda r: len(r["response"]))["response"]
        elif aggregation == "longest":
            aggregated = max(responses, key=lambda r: len(r["response"]))["response"]
        else:
            # Promedio ponderado por tokens
            aggregated = responses[0]["response"] if responses else ""
        
        return {
            "individual_responses": responses,
            "aggregated_response": aggregated,
            "models_used": models,
            "strategy": aggregation,
            "mistral_instruct_used": any(r["is_instruct"] for r in responses)
        }

    def get_model_info(self, model: str = None) -> dict:
        """
        ğŸ“Š Obtener informaciÃ³n de modelo(s)
        
        Args:
            model: Nombre del modelo (None = todos)
        
        Returns:
            dict: InformaciÃ³n del modelo o todos los modelos
        """
        if model:
            return AVAILABLE_MODELS.get(model, {})
        return AVAILABLE_MODELS

    def generate_with_mistral_instruct(
        self,
        instruction: str,
        context: str = "",
        temperature: float = 0.3,
        max_tokens: int = 4000
    ) -> dict:
        """
        ğŸ¯ GeneraciÃ³n especializada con Mistral Instruct
        Optimizado para seguir instrucciones complejas y tareas de ML
        
        Args:
            instruction: InstrucciÃ³n clara a seguir
            context: Contexto adicional
            temperature: Creatividad (0.3 = mÃ¡s determinista)
            max_tokens: Tokens mÃ¡ximos
        
        Returns:
            dict: Respuesta con metadata extendida
        """
        # Construir prompt optimizado para Mistral Instruct
        prompt = f"[INST] {instruction}"
        if context:
            prompt += f"\n\nContext: {context}"
        prompt += " [/INST]"
        
        result = self.generate(
            prompt=prompt,
            model="mistral:instruct",
            temperature=temperature,
            max_tokens=max_tokens,
            use_cache=True,
            use_cognitive_influence=True
        )
        
        # Agregar metadata de Mistral Instruct
        result["model_type"] = "mistral_instruct"
        result["optimal_for"] = ["ml_training", "code_generation", "system_commands"]
        result["instruction_following"] = True
        
        return result


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SINGLETON GLOBAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


def get_ollama_integration(**kwargs) -> "MilitaryGradeOllamaIntegration":
    """Obtiene instancia singleton de MilitaryGradeOllamaIntegration"""
    global _global_ollama_integration

    if _global_ollama_integration is None:
        _global_ollama_integration = MilitaryGradeOllamaIntegration(**kwargs)

    return _global_ollama_integration


# Alias para compatibilidad
OllamaIntegration = MilitaryGradeOllamaIntegration


def test_ollama_integration():
    """Test de la integraciÃ³n"""
    print("\n" + "=" * 60)
    print("ğŸ¤– OLLAMA INTEGRATION - Test")
    print("=" * 60 + "\n")

    # Crear integraciÃ³n
    ollama = get_ollama_integration()

    # Health check
    health = ollama.health_check()
    print("ğŸ¥ Health Check:")
    print(json.dumps(health, indent=2))

    if not ollama.available:
        print("\nâŒ Ollama no estÃ¡ disponible. AsegÃºrate de que estÃ¡ corriendo.")
        return

    # Test de generaciÃ³n
    print("\nğŸ§ª Test de generaciÃ³n:")
    result = ollama.generate(
        "Â¿QuÃ© es el Machine Learning en una frase?", temperature=0.5, max_tokens=100
    )

    if result["success"]:
        print(f"âœ… Respuesta: {result['response'][:200]}...")
        print(f"â±ï¸ Tiempo: {result['response_time_ms']:.2f}ms")
        print(f"ğŸ“ Tokens: {result['tokens']}")
    else:
        print(f"âŒ Error: {result['error']}")

    # Test de chat
    print("\nğŸ’¬ Test de chat:")
    chat_result = ollama.chat([{"role": "user", "content": "Hola, Â¿cÃ³mo estÃ¡s?"}])

    if chat_result["success"]:
        print(f"âœ… Respuesta: {chat_result['response'][:200]}...")
        print(f"â±ï¸ Tiempo: {chat_result['response_time_ms']:.2f}ms")
    else:
        print(f"âŒ Error: {chat_result['error']}")

    # MÃ©tricas
    print("\nğŸ“Š MÃ©tricas:")
    metrics = ollama.get_metrics()
    print(json.dumps(metrics["metrics"], indent=2))

    # Guardar mÃ©tricas
    ollama.save_metrics()

    print("\nâœ… Test completado")


if __name__ == "__main__":
    test_ollama_integration()