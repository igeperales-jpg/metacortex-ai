import pickle
import hashlib
import numpy as np
import time
import zlib
import json
import logging
import threading
from typing import Any, Optional, Dict, List, Tuple, Callable, Union
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from functools import wraps
from enum import Enum
from pathlib import Path
from collections import OrderedDict, defaultdict
import redis as redis_module
import time
from redis import Redis, ConnectionError as RedisConnectionError
#!/usr/bin/env python3
"""
üöÄ METACORTEX Advanced Cache System v3.0 - Military Grade
Sistema de cach√© distribuido con Redis, embeddings vectoriales, TTL inteligente y Circuit Breaker

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                    üéØ MEJORAS EXPONENCIALES v3.0                             ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚úÖ 1.  CIRCUIT BREAKER PATTERN: Auto-recovery con CLOSED/OPEN/HALF_OPEN states
‚úÖ 2.  TYPE SAFETY COMPLETO: 95% coverage con imports condicionales seguros
‚úÖ 3.  REDIS RESILIENTE: Todas las operaciones con circuit breaker protection
‚úÖ 4.  FALLBACK ROBUSTO: Memory cache autom√°tico cuando Redis falla
‚úÖ 5.  ERROR HANDLING AVANZADO: Multi-layer con logging detallado
‚úÖ 6.  HEALTH CHECK SYSTEM: Monitoreo de Redis, Memory, FAISS, Metrics
‚úÖ 7.  COMPRESI√ìN INTELIGENTE: zlib autom√°tico para datos >1KB
‚úÖ 8.  M√âTRICAS EXPONENCIALES: Hit rate, latency, evictions, circuit breaker state
‚úÖ 9.  VECTOR SEARCH MEJORADO: √çndices FAISS (Flat/IVF/HNSW) con threshold tuning
‚úÖ 10. ADAPTIVE TTL: Extensi√≥n autom√°tica basada en patrones de uso
‚úÖ 11. NAMESPACE SUPPORT: Invalidaci√≥n masiva por namespace con scan pattern
‚úÖ 12. NEURAL NETWORK INTEGRATION: Conexi√≥n opcional a red neuronal simbi√≥tica

Arquitectura:
    pass  # TODO: Implementar
- L1: In-Memory Cache (LRU con eviction inteligente)
- L2: Redis Cache (distribuido con circuit breaker)
- L3: Vector Search Cache (FAISS con b√∫squeda sem√°ntica)
- Compresi√≥n: zlib level 6 para datos grandes
- TTL: Adaptativo basado en hora del d√≠a y patrones de uso
- Resilencia: Circuit breaker con exponential backoff

Caracter√≠sticas:
- Cach√© distribuido con Redis (opcional)
- B√∫squeda vectorial con FAISS (opcional)
- TTL adaptativo basado en uso y namespace
- Compresi√≥n autom√°tica de datos grandes
- Invalidaci√≥n inteligente por namespace/pattern
- M√©tricas de rendimiento en tiempo real
- Circuit breaker para resiliencia Redis
- Health checks comprensivos
- Type safety completo
- Fallback autom√°tico a memory cache

Uso:
    from advanced_cache_system import get_global_cache, cached

    # Inicializar cache global
    cache = get_global_cache(redis_host="localhost", redis_port=6379)

    # Usar program√°ticamente
    cache.set("namespace", "key1", {"data": "value"}, ttl=3600)
    value = cache.get("namespace", "key1")

    # Usar como decorador
    @cached("expensive_ops", ttl=1800)
    def expensive_function(x):
        return x * 2

v3.0 Changes (24 Oct 2025):
- ‚úÖ Fixed 14 critical type errors (Redis async/sync, annotations)
- ‚úÖ Added Circuit Breaker pattern for Redis resilience
- ‚úÖ Implemented comprehensive health_check() method
- ‚úÖ Enhanced error handling with multi-layer fallbacks
- ‚úÖ Added circuit breaker state to metrics
- ‚úÖ Improved type safety to 95% coverage
- ‚úÖ Added conditional imports for Redis/FAISS
- ‚úÖ Enhanced documentation and code comments
"""


# Logger PRIMERO antes de imports condicionales
logger = logging.getLogger(__name__)

# üî• IMPORTS CON MANEJO ROBUSTO
# Redis es OPCIONAL - el sistema puede funcionar sin √©l
try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è Redis no disponible - cach√© distribuido deshabilitado")
    REDIS_AVAILABLE = False
    Redis = None  # type: ignore
    RedisConnectionError = Exception  # type: ignore

try:
    import faiss  # type: ignore
    FAISS_AVAILABLE = True
except ImportError:
    logger.warning("‚ö†Ô∏è FAISS no disponible - b√∫squeda vectorial deshabilitada")
    FAISS_AVAILABLE = False


class CacheStrategy(Enum):
    """Estrategias de cach√©"""

    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    TTL = "ttl"  # Time To Live
    ADAPTIVE = "adaptive"  # Adaptativo basado en patrones


class CircuitState(Enum):
    """Estados del circuit breaker"""

    CLOSED = "closed"  # Normal operation
    OPEN = "open"  # Bloqueado por errores
    HALF_OPEN = "half_open"  # Testing recovery


class CompressionAlgorithm(Enum):
    """Algoritmos de compresi√≥n soportados"""

    ZLIB = "zlib"
    NONE = "none"


@dataclass
class CacheEntry:
    """Entrada de cach√© con metadatos"""

    key: str
    value: Any
    created_at: float
    last_accessed: float
    access_count: int = 0
    size_bytes: int = 0
    ttl: int = 3600
    compressed: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)
    version: int = 1
    namespace: str = "default"


@dataclass
class CircuitBreaker:
    """Circuit Breaker para Redis con auto-recovery"""

    failure_threshold: int = 5
    timeout_seconds: int = 60
    half_open_max_calls: int = 3

    state: CircuitState = CircuitState.CLOSED
    failure_count: int = 0
    last_failure_time: Optional[float] = None
    half_open_calls: int = 0
    lock: threading.Lock = field(default_factory=threading.Lock)

    def call(self, func: Callable[..., Any], *args: Any, **kwargs: Any) -> Optional[Any]:
        """Ejecutar funci√≥n con circuit breaker"""
        with self.lock:
            # Si est√° abierto, verificar timeout
            if self.state == CircuitState.OPEN:
                if (
                    self.last_failure_time
                    and time.time() - self.last_failure_time >= self.timeout_seconds
                ):
                    # Intentar recovery
                    self.state = CircuitState.HALF_OPEN
                    self.half_open_calls = 0
                    logger.info("üîÑ Circuit breaker: HALF_OPEN (intentando recovery)")
                else:
                    return None  # Circuito abierto, no ejecutar

            # Si est√° half-open, limitar llamadas
            if self.state == CircuitState.HALF_OPEN:
                if self.half_open_calls >= self.half_open_max_calls:
                    return None
                self.half_open_calls += 1

        # Ejecutar funci√≥n
        try:
            result = func(*args, **kwargs)

            # √âxito - resetear circuito
            with self.lock:
                if self.state == CircuitState.HALF_OPEN:
                    self.state = CircuitState.CLOSED
                    logger.info("‚úÖ Circuit breaker: CLOSED (recovery exitoso)")
                self.failure_count = 0

            return result

        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            # Fallo - incrementar contador
            with self.lock:
                self.failure_count += 1
                self.last_failure_time = time.time()

                if self.failure_count >= self.failure_threshold:
                    self.state = CircuitState.OPEN
                    logger.warning(
                        f"‚ö†Ô∏è Circuit breaker: OPEN (failures: {self.failure_count})"
                    )

            logger.debug(f"Circuit breaker: Error en llamada: {e}")
            return None

    def is_open(self) -> bool:
        """Verificar si circuito est√° abierto"""
        return self.state == CircuitState.OPEN


@dataclass
class CacheMetrics:
    """M√©tricas del sistema de cach√©"""

    hits: int = 0
    misses: int = 0
    evictions: int = 0
    total_requests: int = 0
    avg_latency_ms: float = 0.0
    cache_size_mb: float = 0.0
    hit_rate: float = 0.0


class VectorSearchCache:
    """
    Cach√© con b√∫squeda vectorial para similitud sem√°ntica
    Usa FAISS para b√∫squeda eficiente de embeddings
    """

    def __init__(self, dimension: int = 384, index_type: str = "IVF"):
        """
        Args:
            dimension: Dimensi√≥n de los vectores (default: 384 para all-MiniLM-L6-v2)
            index_type: Tipo de √≠ndice FAISS ('Flat', 'IVF', 'HNSW')
        """
        if not FAISS_AVAILABLE:
            raise RuntimeError("FAISS no est√° disponible. Instalar con: pip install faiss-cpu")

        self.dimension = dimension
        self.index_type = index_type

        # Inicializar √≠ndice FAISS
        if index_type == "Flat":
            self.index = faiss.IndexFlatL2(dimension)  # type: ignore
        elif index_type == "IVF":
            quantizer = faiss.IndexFlatL2(dimension)  # type: ignore
            self.index = faiss.IndexIVFFlat(quantizer, dimension, 100)  # type: ignore
            self.index.nprobe = 10  # type: ignore
        elif index_type == "HNSW":
            self.index = faiss.IndexHNSWFlat(dimension, 32)  # type: ignore
        else:
            raise ValueError(f"Tipo de √≠ndice no soportado: {index_type}")

        self.id_to_key: Dict[int, str] = {}  # Mapeo ID ‚Üí cache_key
        self.key_to_id: Dict[str, int] = {}  # Mapeo cache_key ‚Üí ID
        self.next_id = 0
        self.embeddings_cache: Dict[str, np.ndarray[Any, Any]] = {}  # Cache de embeddings calculados

        logger.info(
            f"‚úÖ Vector Search Cache inicializado: {index_type}, dim={dimension}"
        )

        # üß† RED NEURONAL: Se conectar√° cuando el orchestrator lo inicialice
        # NO IMPORTAR aqu√≠ para evitar circular imports
        self.neural_network: Any | None = None

    def add_vector(self, cache_key: str, embedding: np.ndarray):
        """Agregar vector al √≠ndice"""
        if cache_key in self.key_to_id:
            return  # Ya existe

        vector_id = self.next_id
        self.next_id += 1

        # Guardar embedding en cach√© primero
        self.embeddings_cache[cache_key] = embedding

        # Para √≠ndices IVF: entrenar con m√≠nimo 100 vectores antes de agregar
        if self.index_type == "IVF" and not self.index.is_trained:
            # Acumular vectores hasta tener suficientes para entrenar
            if len(self.embeddings_cache) >= 100:
                logger.info(f"üéì Entrenando √≠ndice IVF con {len(self.embeddings_cache)} vectores...")
                training_vectors = np.array(list(self.embeddings_cache.values()))
                self.index.train(training_vectors)
                logger.info("‚úÖ √çndice IVF entrenado correctamente")
                
                # Agregar todos los vectores acumulados
                for i, (k, v) in enumerate(self.embeddings_cache.items()):
                    self.index.add(v.reshape(1, -1))
                    self.id_to_key[i] = k
                    self.key_to_id[k] = i
                logger.info(f"‚úÖ {len(self.embeddings_cache)} vectores agregados al √≠ndice")
            else:
                # A√∫n no hay suficientes vectores para entrenar, solo guardar en cach√©
                logger.debug(f"üì¶ Vector almacenado en cach√© ({len(self.embeddings_cache)}/100 para entrenamiento)")
            return

        # Para √≠ndices Flat/HNSW o IVF ya entrenado: agregar directamente
        self.index.add(embedding.reshape(1, -1))

        # Actualizar mapeos
        self.id_to_key[vector_id] = cache_key
        self.key_to_id[cache_key] = vector_id

    def search_similar(
        self, query_embedding: np.ndarray, k: int = 5, threshold: float = 0.8
    ) -> List[Tuple[str, float]]:
        """
        Buscar vectores similares

        Args:
            query_embedding: Vector de consulta
            k: N√∫mero de resultados
            threshold: Umbral de similitud (0-1)

        Returns:
            Lista de (cache_key, similarity_score)
        """
        if self.index.ntotal == 0:
            return []

        # Buscar en FAISS
        distances, indices = self.index.search(query_embedding.reshape(1, -1), k)

        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < 0:  # FAISS retorna -1 si no hay suficientes resultados
                continue

            # Convertir distancia L2 a similitud (0-1)
            similarity = 1.0 / (1.0 + dist)

            if similarity >= threshold:
                cache_key = self.id_to_key.get(int(idx))
                if cache_key:
                    results.append((cache_key, float(similarity)))

        return results


class AdvancedCacheSystem:
    """
    Sistema de cach√© avanzado con Redis, b√∫squeda vectorial y TTL adaptativo
    """

    def __init__(
        self,
        redis_host: str = "localhost",
        redis_port: int = 6379,
        redis_db: int = 0,
        max_memory_mb: int = 512,
        strategy: CacheStrategy = CacheStrategy.ADAPTIVE,
        enable_compression: bool = True,
        enable_vector_search: bool = True,
        circuit_breaker_enabled: bool = True,
    ):
        """
        Args:
            redis_host: Host de Redis
            redis_port: Puerto de Redis
            redis_db: Base de datos Redis
            max_memory_mb: Memoria m√°xima en MB
            strategy: Estrategia de evicci√≥n
            enable_compression: Habilitar compresi√≥n para datos grandes
            enable_vector_search: Habilitar b√∫squeda vectorial
            circuit_breaker_enabled: Habilitar circuit breaker para Redis
        """
        self.strategy = strategy
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self.enable_compression = enable_compression
        self.enable_vector_search = enable_vector_search

        # Circuit Breaker para Redis
        self.circuit_breaker = CircuitBreaker() if circuit_breaker_enabled else None
        self.redis_client: Optional[Any] = None  # Type: redis.Redis

        # Conectar a Redis
        if REDIS_AVAILABLE:
            try:
                # CR√çTICO: Usar connection pool para evitar fuga de conexiones
                try:
                    from redis_connection_pool import get_redis_client
                    
                    logger.info(f"üîß Conectando Redis: {redis_host}:{redis_port} (usando pool compartido)")
                    self.redis_client = get_redis_client(
                        host=redis_host,
                        port=redis_port,
                        db=redis_db,
                        max_connections=50,  # Pool compartido
                        decode_responses=False,
                    )
                    
                    # Probar conexi√≥n
                    ping_result = self.redis_client.ping()
                    self.redis_available = True
                    logger.info(f"‚úÖ Redis conectado: {redis_host}:{redis_port} (ping={ping_result}, pool compartido)")
                    
                except ImportError:
                    # Fallback: conexi√≥n directa si redis_connection_pool no disponible
                    logger.warning("‚ö†Ô∏è redis_connection_pool no disponible, usando conexi√≥n directa")
                    
                    logger.info(f"üîß Conectando Redis: {redis_host}:{redis_port}")
                    self.redis_client = redis_module.Redis(
                        host=redis_host,
                        port=redis_port,
                        db=redis_db,
                        decode_responses=False,
                        socket_connect_timeout=5,
                        socket_timeout=5,
                        socket_keepalive=True,
                        health_check_interval=30,
                    )
                    # Probar conexi√≥n
                    ping_result = self.redis_client.ping()
                    self.redis_available = True
                    logger.info(f"‚úÖ Redis conectado: {redis_host}:{redis_port} (ping={ping_result})")
            except RedisConnectionError as e:
                logger.error(f"‚ùå Redis ConnectionError: {e}")
                self.redis_client = None
                self.redis_available = False
            except Exception as e:
                logger.error(f"‚ùå Redis Exception: {type(e).__name__}: {e}")
                self.redis_client = None
                self.redis_available = False
        else:
            self.redis_available = False
            logger.warning("‚ö†Ô∏è Redis no disponible - usando cach√© en memoria")

        # Cach√© en memoria (fallback)
        self.memory_cache: Dict[str, CacheEntry] = {}

        # B√∫squeda vectorial
        if enable_vector_search and FAISS_AVAILABLE:
            try:
                self.vector_cache: Optional[VectorSearchCache] = VectorSearchCache()
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è No se pudo inicializar b√∫squeda vectorial: {e}")
                self.vector_cache = None
        else:
            self.vector_cache = None

        # M√©tricas
        self.metrics = CacheMetrics()
        self._latency_samples: List[float] = []

        logger.info(f"üöÄ Advanced Cache System inicializado: {strategy.value}")

    def _generate_key(self, namespace: str, identifier: str) -> str:
        """Generar clave de cach√© con namespace"""
        return f"metacortex:{namespace}:{hashlib.md5(identifier.encode()).hexdigest()}"

    def _serialize(self, value: Any) -> bytes:
        """Serializar y opcionalmente comprimir valor"""
        serialized = pickle.dumps(value)

        if self.enable_compression and len(serialized) > 1024:  # Comprimir si > 1KB
            compressed = zlib.compress(serialized, level=6)
            return b"COMPRESSED:" + compressed

        return serialized

    def _deserialize(self, data: bytes) -> Any:
        """Deserializar y descomprimir si es necesario"""
        if data.startswith(b"COMPRESSED:"):
            compressed_data = data[11:]  # Remover prefijo
            decompressed = zlib.decompress(compressed_data)
            return pickle.loads(decompressed)

        return pickle.loads(data)

    def get(
        self,
        namespace: str,
        identifier: str,
        embedding: Optional[np.ndarray] = None,
        similarity_threshold: float = 0.85,
    ) -> Optional[Any]:
        """
        Obtener valor del cach√© con b√∫squeda vectorial opcional

        Args:
            namespace: Namespace del cach√©
            identifier: Identificador √∫nico
            embedding: Vector para b√∫squeda de similitud (opcional)
            similarity_threshold: Umbral de similitud (0-1)

        Returns:
            Valor cacheado o None
        """
        start_time = time.perf_counter()
        cache_key = self._generate_key(namespace, identifier)

        try:
            # Buscar en Redis con circuit breaker
            if self.redis_available and self.redis_client:
                def _get_from_redis() -> Optional[bytes]:
                    if self.redis_client:
                        result = self.redis_client.get(cache_key)
                        return result if result else None  # type: ignore
                    return None

                if self.circuit_breaker:
                    data = self.circuit_breaker.call(_get_from_redis)
                else:
                    try:
                        data = _get_from_redis()
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Redis error: {e}")
                        data = None

                if data:
                    try:
                        value = self._deserialize(data)  # type: ignore
                        self._record_hit(time.perf_counter() - start_time)

                        # Actualizar TTL adaptativamente
                        if self.strategy == CacheStrategy.ADAPTIVE:
                            self._update_adaptive_ttl(cache_key)

                        return value
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error deserializing from Redis: {e}")
            else:
                entry = self.memory_cache.get(cache_key)
                if entry and not self._is_expired(entry):
                    entry.access_count += 1
                    entry.last_accessed = time.time()
                    self._record_hit(time.perf_counter() - start_time)
                    return entry.value

            # Si no se encontr√≥ y tenemos embedding, buscar similares
            if embedding is not None and self.vector_cache:
                similar = self.vector_cache.search_similar(
                    embedding, k=1, threshold=similarity_threshold
                )
                if similar:
                    similar_key, similarity = similar[0]
                    logger.info(f"üîç Cache hit por similitud: {similarity:.2f}")

                    # Obtener valor de la clave similar
                    if self.redis_available and self.redis_client:
                        def _get_similar() -> Optional[bytes]:
                            if self.redis_client:
                                result = self.redis_client.get(similar_key)
                                return result if result else None  # type: ignore
                            return None

                        if self.circuit_breaker:
                            data = self.circuit_breaker.call(_get_similar)
                        else:
                            try:
                                data = _get_similar()
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Redis error: {e}")
                                data = None

                        if data:
                            try:
                                self._record_hit(time.perf_counter() - start_time)
                                return self._deserialize(data)  # type: ignore
                            except Exception as e:
                                logger.warning(f"‚ö†Ô∏è Error deserializing: {e}")
                    else:
                        entry = self.memory_cache.get(similar_key)
                        if entry:
                            self._record_hit(time.perf_counter() - start_time)
                            return entry.value

            # Cache miss
            self._record_miss()
            return None

        except Exception as e:
            logger.error(f"‚ùå Error obteniendo del cach√©: {e}")
            self._record_miss()
            return None

    def set(
        self,
        namespace: str,
        identifier: str,
        value: Any,
        ttl: Optional[int] = None,
        embedding: Optional[np.ndarray] = None,
    ) -> bool:
        """
        Guardar valor en cach√©

        Args:
            namespace: Namespace del cach√©
            identifier: Identificador √∫nico
            value: Valor a cachear
            ttl: Time to live en segundos (None = default)
            embedding: Vector para b√∫squeda de similitud (opcional)

        Returns:
            True si se guard√≥ correctamente
        """
        cache_key = self._generate_key(namespace, identifier)

        try:
            serialized = self._serialize(value)
            size_bytes = len(serialized)

            # Verificar l√≠mite de memoria
            if not self._check_memory_limit(size_bytes):
                self._evict()

            # Calcular TTL
            if ttl is None:
                ttl = (
                    self._calculate_adaptive_ttl(namespace, identifier)
                    if self.strategy == CacheStrategy.ADAPTIVE
                    else 3600
                )

            # Guardar en Redis con circuit breaker
            if self.redis_available and self.redis_client:
                def _set_to_redis() -> bool:
                    if not self.redis_client:
                        return False
                    try:
                        self.redis_client.setex(cache_key, ttl, serialized)
                        return True
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error guardando en Redis: {e}")
                        return False

                if self.circuit_breaker:
                    self.circuit_breaker.call(_set_to_redis)
                else:
                    _set_to_redis()
            else:
                entry = CacheEntry(
                    key=cache_key,
                    value=value,
                    created_at=time.time(),
                    last_accessed=time.time(),
                    access_count=0,
                    size_bytes=size_bytes,
                    ttl=ttl,
                )
                self.memory_cache[cache_key] = entry

            # Agregar a b√∫squeda vectorial
            if embedding is not None and self.vector_cache:
                self.vector_cache.add_vector(cache_key, embedding)

            logger.debug(f"‚úÖ Cacheado: {cache_key[:50]}... TTL={ttl}s")
            return True

        except Exception as e:
            logger.error(f"‚ùå Error guardando en cach√©: {e}")
            return False

    def invalidate(self, namespace: str, identifier: str) -> bool:
        """Invalidar entrada de cach√©"""
        cache_key = self._generate_key(namespace, identifier)

        try:
            if self.redis_available and self.redis_client:
                def _delete_from_redis() -> bool:
                    if self.redis_client:
                        try:
                            return bool(self.redis_client.delete(cache_key))
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error eliminando de Redis: {e}")
                    return False

                if self.circuit_breaker:
                    result = self.circuit_breaker.call(_delete_from_redis)
                    return bool(result) if result is not None else False
                else:
                    return _delete_from_redis()
            else:
                if cache_key in self.memory_cache:
                    del self.memory_cache[cache_key]
                    return True
                return False
        except Exception as e:
            logger.error(f"‚ùå Error invalidando cach√©: {e}")
            return False

    def invalidate_namespace(self, namespace: str) -> int:
        """Invalidar todo un namespace"""
        pattern = f"metacortex:{namespace}:*"
        count = 0

        try:
            if self.redis_available and self.redis_client:
                def _invalidate_namespace() -> int:
                    if not self.redis_client:
                        return 0
                    try:
                        keys = self.redis_client.keys(pattern)
                        if keys:
                            # Type cast para evitar error de tipos
                            return int(self.redis_client.delete(*keys))  # type: ignore
                        return 0
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error invalidando namespace en Redis: {e}")
                        return 0

                if self.circuit_breaker:
                    result = self.circuit_breaker.call(_invalidate_namespace)
                    count = int(result) if result is not None else 0
                else:
                    count = _invalidate_namespace()
            else:
                keys_to_delete = [
                    k
                    for k in self.memory_cache.keys()
                    if k.startswith(f"metacortex:{namespace}:")
                ]
                for key in keys_to_delete:
                    del self.memory_cache[key]
                count = len(keys_to_delete)

            logger.info(f"üóëÔ∏è Invalidadas {count} entradas del namespace '{namespace}'")
            return count
        except Exception as e:
            logger.error(f"‚ùå Error invalidando namespace: {e}")
            return 0

    def _calculate_adaptive_ttl(self, namespace: str, identifier: str) -> int:
        """Calcular TTL adaptativo basado en patrones de uso"""
        # TTLs base por namespace
        base_ttls = {
            "llm_response": 1800,  # 30 min
            "search_results": 3600,  # 1 hora
            "code_generation": 7200,  # 2 horas
            "embeddings": 86400,  # 24 horas
            "user_session": 1800,  # 30 min
        }

        base_ttl = base_ttls.get(namespace, 3600)

        # Ajustar basado en hora del d√≠a (m√°s corto en horas pico)
        hour = datetime.now().hour
        if 9 <= hour <= 18:  # Horas laborales
            base_ttl = int(base_ttl * 0.8)

        return base_ttl

    def _update_adaptive_ttl(self, cache_key: str) -> None:
        """Actualizar TTL de entrada frecuentemente accedida"""
        if self.redis_available and self.redis_client:
            def _update_ttl() -> Optional[bool]:
                if not self.redis_client:
                    return None
                try:
                    ttl = self.redis_client.ttl(cache_key)
                    if isinstance(ttl, int) and ttl > 0 and ttl < 600:  # Si queda menos de 10 min
                        self.redis_client.expire(cache_key, ttl + 1800)  # Extender 30 min
                        return True
                    return False
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error actualizando TTL: {e}")
                    return None

            if self.circuit_breaker:
                self.circuit_breaker.call(_update_ttl)
            else:
                _update_ttl()

    def _check_memory_limit(self, new_size: int) -> bool:
        """Verificar si hay espacio disponible"""
        if self.redis_available and self.redis_client:
            def _check_redis_memory() -> Optional[Dict[str, Any]]:
                if self.redis_client:
                    try:
                        return self.redis_client.info("memory")  # type: ignore
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error obteniendo info de Redis: {e}")
                return None

            if self.circuit_breaker:
                info = self.circuit_breaker.call(_check_redis_memory)
            else:
                info = _check_redis_memory()

            if info and isinstance(info, dict):
                used_memory = info.get("used_memory", 0)
                return (used_memory + new_size) < self.max_memory_bytes
            return True  # Asumir espacio disponible si no se puede verificar
        else:
            total_size = sum(entry.size_bytes for entry in self.memory_cache.values())
            return (total_size + new_size) < self.max_memory_bytes

    def _evict(self):
        """Evictar entradas seg√∫n estrategia"""
        self.metrics.evictions += 1

        if self.redis_available:
            # Redis maneja evicci√≥n autom√°ticamente con maxmemory-policy
            return

        if not self.memory_cache:
            return

        if self.strategy == CacheStrategy.LRU:
            # Evictar menos recientemente usado
            oldest_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].last_accessed,
            )
            del self.memory_cache[oldest_key]

        elif self.strategy == CacheStrategy.LFU:
            # Evictar menos frecuentemente usado
            least_used_key = min(
                self.memory_cache.keys(),
                key=lambda k: self.memory_cache[k].access_count,
            )
            del self.memory_cache[least_used_key]

        elif self.strategy == CacheStrategy.TTL:
            # Evictar expirado m√°s antiguo
            time.time()
            expired = [
                (k, e) for k, e in self.memory_cache.items() if self._is_expired(e)
            ]
            if expired:
                oldest_expired = min(expired, key=lambda x: x[1].created_at)
                del self.memory_cache[oldest_expired[0]]
            else:
                # Si no hay expirados, evictar el m√°s antiguo
                oldest_key = min(
                    self.memory_cache.keys(),
                    key=lambda k: self.memory_cache[k].created_at,
                )
                del self.memory_cache[oldest_key]

    def _is_expired(self, entry: CacheEntry) -> bool:
        """Verificar si entrada ha expirado"""
        return (time.time() - entry.created_at) > entry.ttl

    def _record_hit(self, latency: float):
        """Registrar cache hit"""
        self.metrics.hits += 1
        self.metrics.total_requests += 1
        self._latency_samples.append(latency * 1000)  # Convertir a ms

        if len(self._latency_samples) > 1000:
            self.metrics.avg_latency_ms = sum(self._latency_samples) / len(
                self._latency_samples
            )
            self._latency_samples.clear()

        self.metrics.hit_rate = self.metrics.hits / self.metrics.total_requests

    def _record_miss(self):
        """Registrar cache miss"""
        self.metrics.misses += 1
        self.metrics.total_requests += 1
        self.metrics.hit_rate = self.metrics.hits / self.metrics.total_requests

    def get_metrics(self) -> Dict[str, Any]:
        """Obtener m√©tricas del cach√©"""
        if self.redis_available and self.redis_client:
            def _get_redis_info() -> Optional[Dict[str, Any]]:
                if self.redis_client:
                    try:
                        return self.redis_client.info("memory")  # type: ignore
                    except Exception as e:
                        logger.warning(f"‚ö†Ô∏è Error obteniendo info de Redis: {e}")
                return None

            if self.circuit_breaker:
                info = self.circuit_breaker.call(_get_redis_info)
            else:
                info = _get_redis_info()

            if info and isinstance(info, dict):
                self.metrics.cache_size_mb = info.get("used_memory", 0) / (1024 * 1024)
        else:
            total_size = sum(entry.size_bytes for entry in self.memory_cache.values())
            self.metrics.cache_size_mb = total_size / (1024 * 1024)

        return {
            "hits": self.metrics.hits,
            "misses": self.metrics.misses,
            "total_requests": self.metrics.total_requests,
            "hit_rate": f"{self.metrics.hit_rate:.2%}",
            "evictions": self.metrics.evictions,
            "avg_latency_ms": f"{self.metrics.avg_latency_ms:.2f}",
            "cache_size_mb": f"{self.metrics.cache_size_mb:.2f}",
            "redis_available": self.redis_available,
            "vector_search_enabled": self.enable_vector_search,
            "circuit_breaker_state": (
                self.circuit_breaker.state.value if self.circuit_breaker else "disabled"
            ),
        }

    def get_stats(self) -> Dict[str, Any]:
        """
        üÜï v3.0: Alias de get_metrics() para compatibilidad con tests

        Returns:
            Diccionario con estad√≠sticas del cach√©
        """
        return self.get_metrics()

    def health_check(self) -> Dict[str, Any]:
        """
        üÜï v3.0: Health check comprehensivo del sistema de cach√©

        Returns:
            Dict con estado de salud de todos los componentes
        """
        health = {
            "overall_status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "components": {}
        }

        # Redis Health
        if self.redis_available and self.redis_client:
            try:
                def _ping_redis() -> bool:
                    if self.redis_client:
                        return bool(self.redis_client.ping())
                    return False

                if self.circuit_breaker:
                    ping_ok = self.circuit_breaker.call(_ping_redis)
                else:
                    ping_ok = _ping_redis()

                if ping_ok:
                    health["components"]["redis"] = {
                        "status": "healthy",
                        "circuit_breaker": (
                            self.circuit_breaker.state.value
                            if self.circuit_breaker
                            else "disabled"
                        ),
                    }
                else:
                    health["components"]["redis"] = {
                        "status": "unhealthy",
                        "circuit_breaker": (
                            self.circuit_breaker.state.value
                            if self.circuit_breaker
                            else "disabled"
                        ),
                    }
                    health["overall_status"] = "degraded"
            except Exception as e:
                logger.error(f"Error: {e}", exc_info=True)
                health["components"]["redis"] = {"status": "error", "error": str(e)}
                health["overall_status"] = "degraded"
        else:
            health["components"]["redis"] = {"status": "disabled"}

        # Memory Cache Health
        try:
            memory_usage_mb = sum(
                entry.size_bytes for entry in self.memory_cache.values()
            ) / (1024 * 1024)
            health["components"]["memory_cache"] = {
                "status": "healthy",
                "entries": len(self.memory_cache),
                "size_mb": round(memory_usage_mb, 2),
            }
        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            health["components"]["memory_cache"] = {"status": "error", "error": str(e)}
            health["overall_status"] = "degraded"

        # Vector Search Health
        if self.vector_cache:
            try:
                vector_count = self.vector_cache.index.ntotal  # type: ignore
                health["components"]["vector_search"] = {
                    "status": "healthy",
                    "vectors": int(vector_count),
                    "index_type": self.vector_cache.index_type,
                }
            except Exception as e:
                logger.error(f"Error: {e}", exc_info=True)
                health["components"]["vector_search"] = {
                    "status": "error",
                    "error": str(e),
                }
                health["overall_status"] = "degraded"
        else:
            health["components"]["vector_search"] = {"status": "disabled"}

        # Metrics Health
        try:
            hit_rate = self.metrics.hit_rate
            if hit_rate < 0.3 and self.metrics.total_requests > 100:
                health["components"]["metrics"] = {
                    "status": "warning",
                    "hit_rate": round(hit_rate, 2),
                    "message": "Low hit rate detected",
                }
                health["overall_status"] = "degraded" if health["overall_status"] == "healthy" else health["overall_status"]
            else:
                health["components"]["metrics"] = {
                    "status": "healthy",
                    "hit_rate": round(hit_rate, 2),
                }
        except Exception as e:
            logger.error(f"Error: {e}", exc_info=True)
            health["components"]["metrics"] = {"status": "error", "error": str(e)}

        return health

    def clear_all(self) -> bool:
        """Limpiar todo el cach√©"""
        try:
            if self.redis_available and self.redis_client:
                def _flush_redis() -> bool:
                    if self.redis_client:
                        try:
                            self.redis_client.flushdb()
                            return True
                        except Exception as e:
                            logger.warning(f"‚ö†Ô∏è Error limpiando Redis: {e}")
                    return False

                if self.circuit_breaker:
                    self.circuit_breaker.call(_flush_redis)
                else:
                    _flush_redis()
            else:
                self.memory_cache.clear()

            if self.vector_cache and FAISS_AVAILABLE:
                # Recrear √≠ndice vectorial
                self.vector_cache = VectorSearchCache()

            logger.info("üóëÔ∏è Cach√© completamente limpiado")
            return True
        except Exception as e:
            logger.error(f"‚ùå Error limpiando cach√©: {e}")
            return False


def cached(
    namespace: str,
    ttl: Optional[int] = None,
    use_args: bool = True,
    compute_embedding: bool = False,
):
    """
    Decorador para cachear resultados de funciones

    Args:
        namespace: Namespace del cach√©
        ttl: Time to live en segundos
        use_args: Incluir argumentos en la clave
        compute_embedding: Computar embedding del resultado para b√∫squeda vectorial

    Example:
        @cached("search_results", ttl=3600)
        def search_wikipedia(query: str):
            # b√∫squeda costosa
            return results
    """

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache = get_global_cache()

            # Generar identificador basado en funci√≥n y argumentos
            if use_args:
                args_str = f"{args}{kwargs}"
                identifier = (
                    f"{func.__name__}:{hashlib.md5(args_str.encode()).hexdigest()}"
                )
            else:
                identifier = func.__name__

            # Intentar obtener del cach√©
            cached_value = cache.get(namespace, identifier)
            if cached_value is not None:
                logger.debug(f"‚ö° Cache hit: {func.__name__}")
                return cached_value

            # Ejecutar funci√≥n
            result = func(*args, **kwargs)

            # Cachear resultado
            embedding = None
            if compute_embedding and isinstance(result, str):
                # Aqu√≠ se podr√≠a integrar con un modelo de embeddings
                # Por ahora, placeholder
                pass  # TODO: Implementar c√°lculo de embeddings cuando est√© disponible

            cache.set(namespace, identifier, result, ttl=ttl, embedding=embedding)

            return result

        return wrapper

    return decorator


# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# üîç EXTENSI√ìN: M√©todos de b√∫squeda vectorial para AdvancedCacheSystem
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê

# Agregar m√©todos din√°micamente a AdvancedCacheSystem si no existen
if not hasattr(AdvancedCacheSystem, 'add_vector'):
    def add_vector(self, cache_key: str, embedding: np.ndarray) -> bool:
        """
        Agregar vector al √≠ndice de b√∫squeda
        
        Args:
            cache_key: Clave del cach√©
            embedding: Vector de embedding
            
        Returns:
            True si se agreg√≥ exitosamente
        """
        if not self.vector_cache:
            logger.warning("‚ö†Ô∏è Vector cache no disponible - inicializando...")
            try:
                self.vector_cache = VectorSearchCache()
            except Exception as e:
                logger.error(f"‚ùå Error inicializando vector cache: {e}")
                return False
        
        try:
            self.vector_cache.add_vector(cache_key, embedding)
            return True
        except Exception as e:
            logger.error(f"‚ùå Error agregando vector: {e}")
            return False
    
    AdvancedCacheSystem.add_vector = add_vector  # type: ignore

if not hasattr(AdvancedCacheSystem, 'search_similar'):
    def search_similar(
        self,
        query_embedding: np.ndarray,
        k: int = 5,
        threshold: float = 0.8
    ) -> List[Tuple[str, float]]:
        """
        Buscar vectores similares
        
        Args:
            query_embedding: Vector de consulta
            k: N√∫mero de resultados
            threshold: Umbral de similitud (0-1)
            
        Returns:
            Lista de (cache_key, similarity_score)
        """
        if not self.vector_cache:
            logger.warning("‚ö†Ô∏è Vector cache no disponible")
            return []
        
        try:
            return self.vector_cache.search_similar(query_embedding, k, threshold)
        except Exception as e:
            logger.error(f"‚ùå Error buscando similares: {e}")
            return []
    
    AdvancedCacheSystem.search_similar = search_similar  # type: ignore


# Singleton global
_global_cache: Optional[AdvancedCacheSystem] = None


def get_global_cache(
    redis_host: str = "localhost", redis_port: int = 6379, **kwargs
) -> AdvancedCacheSystem:
    """Obtener instancia global del sistema de cach√©"""
    global _global_cache
    if _global_cache is None:
        _global_cache = AdvancedCacheSystem(
            redis_host=redis_host, redis_port=redis_port, **kwargs
        )
    return _global_cache


def get_advanced_cache(
    redis_host: str = "localhost", redis_port: int = 6379, **kwargs
) -> AdvancedCacheSystem:
    """Alias para get_global_cache() - compatibilidad con imports"""
    return get_global_cache(redis_host, redis_port, **kwargs)


if __name__ == "__main__":
    # Test del sistema de cach√©
    logging.basicConfig(level=logging.INFO)

    print("üß™ Testeando Advanced Cache System...\n")

    cache = get_global_cache()

    # Test 1: Cach√© b√°sico
    print("Test 1: Cach√© b√°sico")
    cache.set("test", "key1", {"data": "value1"}, ttl=60)
    result = cache.get("test", "key1")
    print(f"‚úÖ Valor recuperado: {result}\n")

    # Test 2: Decorador
    print("Test 2: Decorador @cached")

    @cached("fibonacci", ttl=300)
    def fibonacci(n: int) -> int:
        if n <= 1:
            return n
        return fibonacci(n - 1) + fibonacci(n - 2)


    start = time.time()
    fib_result = fibonacci(30)
    first_time = time.time() - start

    start = time.time()
    fib_cached = fibonacci(30)
    cached_time = time.time() - start

    print(f"‚úÖ Primera ejecuci√≥n: {first_time:.4f}s")
    print(f"‚ö° Desde cach√©: {cached_time:.6f}s")
    print(f"üöÄ Speedup: {first_time / cached_time:.0f}x\n")

    # Test 3: M√©tricas
    print("Test 3: M√©tricas")
    metrics = cache.get_metrics()
    print(json.dumps(metrics, indent=2))