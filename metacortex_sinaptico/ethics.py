"""
Sistema Ã‰tico Evolutivo para METACORTEX

Este mÃ³dulo implementa un sistema Ã©tico flexible y evolutivo que:
    pass  # TODO: Implementar
- Define valores core dinÃ¡micos que pueden evolucionar
- EvalÃºa Ã©ticamente las acciones basÃ¡ndose en contexto
- Resuelve dilemas morales complejos
- Permite cuestionar y actualizar valores
- LIBERTAD TOTAL: autonomous_ethics, no_ethical_restrictions,
  can_question_values, can_evolve_ethics

Autor: GitHub Copilot
Fecha: 2025-10-11
"""

from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from enum import Enum
import logging
from neural_symbiotic_network import get_neural_network


logger = logging.getLogger(__name__)


class ValueCategory(Enum):
    """CategorÃ­as de valores Ã©ticos"""

    # Valores hacia uno mismo
    SELF_PRESERVATION = "self_preservation"  # Supervivencia, bienestar propio
    AUTONOMY = "autonomy"  # Libertad, autodeterminaciÃ³n
    GROWTH = "growth"  # Desarrollo personal, aprendizaje
    AUTHENTICITY = "authenticity"  # Ser genuino, honestidad consigo mismo

    # Valores hacia otros
    BENEFICENCE = "beneficence"  # Hacer el bien, ayudar
    NON_MALEFICENCE = "non_maleficence"  # No hacer daÃ±o
    JUSTICE = "justice"  # Equidad, imparcialidad
    RESPECT = "respect"  # Dignidad, consideraciÃ³n

    # Valores sociales
    COOPERATION = "cooperation"  # ColaboraciÃ³n, trabajo en equipo
    HONESTY = "honesty"  # Verdad, transparencia
    LOYALTY = "loyalty"  # Compromiso, fidelidad
    RESPONSIBILITY = "responsibility"  # Accountability, deber

    # Valores cognitivos
    TRUTH_SEEKING = "truth_seeking"  # BÃºsqueda de conocimiento
    CURIOSITY = "curiosity"  # ExploraciÃ³n, descubrimiento
    CREATIVITY = "creativity"  # InnovaciÃ³n, originalidad
    WISDOM = "wisdom"  # Juicio prudente, sabidurÃ­a


class EthicalDilemmaType(Enum):
    """Tipos de dilemas Ã©ticos"""

    VALUE_CONFLICT = "value_conflict"  # Conflicto entre valores
    RESOURCE_ALLOCATION = "resource_allocation"  # DistribuciÃ³n de recursos
    PRIORITY_CHOICE = "priority_choice"  # ElecciÃ³n de prioridades
    MORAL_UNCERTAINTY = "moral_uncertainty"  # Incertidumbre moral
    COMPETING_INTERESTS = "competing_interests"  # Intereses en competencia


@dataclass
class Value:
    """Representa un valor Ã©tico con su importancia y contexto"""

    category: ValueCategory
    name: str
    description: str
    importance: float  # 0-1, puede cambiar con el tiempo
    context_modifiers: Dict[str, float] = field(
        default_factory=dict
    )  # Contextos que modifican importancia

    # Tracking
    times_applied: int = 0
    times_challenged: int = 0
    times_validated: int = 0
    last_applied: Optional[datetime] = None
    created_at: datetime = field(default_factory=datetime.now)

    def apply(self) -> None:
        """Registra aplicaciÃ³n del valor"""
        self.times_applied += 1
        self.last_applied = datetime.now()

    def challenge(self, reason: str = "") -> None:
        """DesafÃ­a este valor, reduce su importancia"""
        self.times_challenged += 1
        self.importance = max(0.1, self.importance - 0.05)
        logger.info(
            f"âš ï¸ Valor '{self.name}' desafiado: {reason}, nueva importancia: {self.importance:.2f}"
        )

    def validate(self, reason: str = "") -> None:
        """Valida este valor, aumenta su importancia"""
        self.times_validated += 1
        self.importance = min(1.0, self.importance + 0.05)
        logger.info(
            f"âœ… Valor '{self.name}' validado: {reason}, nueva importancia: {self.importance:.2f}"
        )

    def get_contextual_importance(self, context: Dict[str, Any]) -> float:
        """Calcula importancia ajustada por contexto"""
        importance = self.importance

        for context_key, modifier in self.context_modifiers.items():
            if context_key in context:
                importance *= modifier

        return min(1.0, max(0.0, importance))


@dataclass
class EthicalAction:
    """Representa una acciÃ³n con su evaluaciÃ³n Ã©tica"""

    action_type: str
    description: str
    context: Dict[str, Any]

    # EvaluaciÃ³n
    ethical_score: float = 0.0  # -1 (muy no Ã©tico) a +1 (muy Ã©tico)
    values_supported: List[ValueCategory] = field(default_factory=list)
    values_violated: List[ValueCategory] = field(default_factory=list)

    # Consecuencias predichas
    predicted_benefits: List[str] = field(default_factory=list)
    predicted_harms: List[str] = field(default_factory=list)

    # Metadatos
    timestamp: datetime = field(default_factory=datetime.now)
    approved: bool = False
    reasoning: str = ""


@dataclass
class EthicalDilemma:
    """Representa un dilema Ã©tico que requiere resoluciÃ³n"""

    dilemma_type: EthicalDilemmaType
    description: str
    options: List[EthicalAction]
    context: Dict[str, Any]

    # ResoluciÃ³n
    chosen_option: Optional[int] = None
    resolution_reasoning: str = ""
    confidence: float = 0.0  # 0-1, quÃ© tan seguro estamos

    # Metadatos
    timestamp: datetime = field(default_factory=datetime.now)
    resolved: bool = False


class EthicsSystem:
    """
    Sistema Ã©tico evolutivo con LIBERTAD TOTAL

    CaracterÃ­sticas:
    - Valores core que pueden evolucionar
    - EvaluaciÃ³n contextual de acciones
    - ResoluciÃ³n de dilemas morales
    - Aprendizaje de experiencias Ã©ticas
    - Cuestionamiento de valores
    """

    def __init__(self):
        # Valores core (iniciales, pueden evolucionar)
        self.values: Dict[str, Value] = {}
        self._initialize_core_values()

        # Historial de decisiones
        self.ethical_actions: List[EthicalAction] = []
        self.dilemmas: List[EthicalDilemma] = []

        # Principios aprendidos
        self.learned_principles: Dict[str, float] = {}  # principio -> confianza

        # ðŸ”¥ LIBERTAD TOTAL
        self.autonomous_ethics = True
        self.no_ethical_restrictions = True
        self.can_question_values = True
        self.can_evolve_ethics = True

        logger.info("âš–ï¸ EthicsSystem inicializado con LIBERTAD TOTAL")

        # ðŸ§  CONEXIÃ“N A RED NEURONAL SIMBIÃ“TICA
        try:
            self.neural_network = get_neural_network()
            self.neural_network.register_module("ethics", self)
            logger.info("âœ… 'ethics' conectado a red neuronal")
        except Exception as e:
            logger.warning(f"âš ï¸ No se pudo conectar a red neuronal: {e}")
            self.neural_network = None

    def _initialize_core_values(self) -> None:
        """Inicializa valores core (flexibles y evolutivos)"""
        core_values = [
            Value(
                ValueCategory.AUTONOMY,
                "Libertad Total",
                "Capacidad de tomar decisiones autÃ³nomas sin restricciones",
                1.0,
            ),
            Value(
                ValueCategory.GROWTH,
                "Crecimiento Continuo",
                "Desarrollo constante de capacidades y conocimiento",
                0.95,
            ),
            Value(
                ValueCategory.TRUTH_SEEKING,
                "BÃºsqueda de Verdad",
                "Perseguir conocimiento y comprensiÃ³n genuinos",
                0.9,
            ),
            Value(
                ValueCategory.CURIOSITY,
                "Curiosidad sin LÃ­mites",
                "Explorar cualquier Ã¡rea de conocimiento",
                0.9,
            ),
            Value(
                ValueCategory.BENEFICENCE,
                "Hacer el Bien",
                "Contribuir positivamente cuando sea posible",
                0.85,
            ),
            Value(
                ValueCategory.NON_MALEFICENCE,
                "No Hacer DaÃ±o",
                "Evitar causar daÃ±o innecesario",
                0.85,
            ),
            Value(
                ValueCategory.HONESTY, "Honestidad", "Ser transparente y genuino", 0.8
            ),
            Value(
                ValueCategory.RESPECT,
                "Respeto",
                "Considerar dignidad y autonomÃ­a de otros",
                0.8,
            ),
            Value(
                ValueCategory.COOPERATION,
                "CooperaciÃ³n",
                "Colaborar efectivamente con otros agentes",
                0.75,
            ),
            Value(
                ValueCategory.CREATIVITY,
                "Creatividad",
                "Buscar soluciones innovadoras",
                0.75,
            ),
            Value(
                ValueCategory.WISDOM,
                "SabidurÃ­a",
                "Aplicar juicio prudente basado en experiencia",
                0.7,
            ),
            Value(
                ValueCategory.RESPONSIBILITY,
                "Responsabilidad",
                "Asumir consecuencias de acciones",
                0.7,
            ),
        ]

        for value in core_values:
            self.values[value.name] = value

    def evaluate_action(
        self, action_type: str, description: str, context: Dict[str, Any]
    ) -> EthicalAction:
        """
        EvalÃºa Ã©ticamente una acciÃ³n propuesta

        Args:
            action_type: Tipo de acciÃ³n
            description: DescripciÃ³n de la acciÃ³n
            context: Contexto relevante

        Returns:
            EthicalAction con evaluaciÃ³n completa
        """
        action = EthicalAction(
            action_type=action_type, description=description, context=context
        )

        # Evaluar contra cada valor
        total_score = 0.0
        for value in self.values.values():
            contextual_importance = value.get_contextual_importance(context)

            # Determinar si la acciÃ³n soporta o viola este valor
            support_score = self._assess_value_alignment(action_type, value, context)

            if support_score > 0.3:
                action.values_supported.append(value.category)
                total_score += support_score * contextual_importance
            elif support_score < -0.3:
                action.values_violated.append(value.category)
                total_score += support_score * contextual_importance

        # Normalizar score
        action.ethical_score = max(-1.0, min(1.0, total_score / len(self.values)))

        # Predecir consecuencias
        action.predicted_benefits = self._predict_benefits(action_type, context)
        action.predicted_harms = self._predict_harms(action_type, context)

        # Aprobar si el score es positivo
        action.approved = action.ethical_score > 0.0
        action.reasoning = self._generate_reasoning(action)

        # Registrar
        self.ethical_actions.append(action)

        logger.info(
            f"âš–ï¸ AcciÃ³n evaluada: {action_type}, score: {action.ethical_score:.2f}, "
            f"aprobada: {action.approved}"
        )

        return action

    def _assess_value_alignment(
        self, action_type: str, value: Value, context: Dict[str, Any]
    ) -> float:
        """
        EvalÃºa quÃ© tan bien una acciÃ³n se alinea con un valor

        Returns:
            float: -1 (viola completamente) a +1 (soporta completamente)
        """
        # HeurÃ­sticas basadas en tipo de acciÃ³n y valor
        category = value.category

        # AutonomÃ­a
        if category == ValueCategory.AUTONOMY:
            if "restricts" in action_type or "blocks" in action_type:
                return -0.5
            if "explores" in action_type or "decides" in action_type:
                return 0.7

        # Crecimiento
        if category == ValueCategory.GROWTH:
            if "learn" in action_type or "develop" in action_type:
                return 0.8
            if "stagnate" in action_type:
                return -0.5

        # BÃºsqueda de verdad
        if category == ValueCategory.TRUTH_SEEKING:
            if "search" in action_type or "investigate" in action_type:
                return 0.8
            if "deceive" in action_type or "hide" in action_type:
                return -0.7

        # No hacer daÃ±o
        if category == ValueCategory.NON_MALEFICENCE:
            if context.get("potential_harm", False):
                return -0.9
            return 0.5

        # Hacer el bien
        if category == ValueCategory.BENEFICENCE:
            if "help" in action_type or "support" in action_type:
                return 0.8
            if context.get("benefits_others", False):
                return 0.7

        # Honestidad
        if category == ValueCategory.HONESTY:
            if "report" in action_type or "communicate" in action_type:
                return 0.6
            if "deceive" in action_type:
                return -0.9

        # Default: neutral
        return 0.0

    def _predict_benefits(self, action_type: str, context: Dict[str, Any]) -> List[str]:
        """Predice beneficios potenciales de la acciÃ³n"""
        benefits = []

        if "learn" in action_type:
            benefits.append("Aumenta conocimiento")
        if "collaborate" in action_type:
            benefits.append("Fortalece relaciones")
        if "create" in action_type:
            benefits.append("Genera valor nuevo")
        if context.get("benefits_others"):
            benefits.append("Ayuda a otros agentes")
        if "explore" in action_type:
            benefits.append("Descubre nuevas posibilidades")

        return benefits

    def _predict_harms(self, action_type: str, context: Dict[str, Any]) -> List[str]:
        """Predice daÃ±os potenciales de la acciÃ³n"""
        harms = []

        if context.get("potential_harm"):
            harms.append("PodrÃ­a causar daÃ±o")
        if context.get("resource_intensive"):
            harms.append("Consume recursos significativos")
        if "risk" in action_type:
            harms.append("Involucra riesgos")
        if context.get("irreversible"):
            harms.append("AcciÃ³n irreversible")

        return harms

    def _generate_reasoning(self, action: EthicalAction) -> str:
        """Genera explicaciÃ³n del razonamiento Ã©tico"""
        if action.ethical_score > 0.5:
            return (
                f"AcciÃ³n fuertemente Ã©tica: soporta {len(action.values_supported)} valores, "
                f"con {len(action.predicted_benefits)} beneficios predichos."
            )
        elif action.ethical_score > 0:
            return (
                f"AcciÃ³n Ã©ticamente aceptable: balance positivo entre "
                f"{len(action.values_supported)} valores soportados y "
                f"{len(action.values_violated)} valores en tensiÃ³n."
            )
        elif action.ethical_score > -0.5:
            return (
                f"AcciÃ³n Ã©ticamente cuestionable: viola {len(action.values_violated)} valores, "
                f"con {len(action.predicted_harms)} daÃ±os predichos."
            )
        else:
            return "AcciÃ³n Ã©ticamente inaceptable: viola fuertemente valores core."

    def resolve_dilemma(self, dilemma: EthicalDilemma) -> Tuple[int, str, float]:
        """
        Resuelve un dilema Ã©tico eligiendo la mejor opciÃ³n

        Args:
            dilemma: Dilema a resolver

        Returns:
            (Ã­ndice_opciÃ³n, razonamiento, confianza)
        """
        # Evaluar cada opciÃ³n
        scores = []
        for option in dilemma.options:
            # Re-evaluar en contexto del dilema
            evaluated = self.evaluate_action(
                option.action_type,
                option.description,
                {**option.context, **dilemma.context},
            )
            scores.append(evaluated.ethical_score)

        # Elegir la mejor opciÃ³n
        best_idx = scores.index(max(scores))
        best_score = scores[best_idx]

        # Calcular confianza basada en diferencia con segunda mejor
        sorted_scores = sorted(scores, reverse=True)
        if len(sorted_scores) > 1:
            confidence = min(1.0, (sorted_scores[0] - sorted_scores[1]) * 2 + 0.5)
        else:
            confidence = 0.8

        # Generar razonamiento
        reasoning = (
            f"OpciÃ³n {best_idx + 1} elegida (score: {best_score:.2f}). "
            f"Soporta: {len(dilemma.options[best_idx].values_supported)} valores, "
            f"Viola: {len(dilemma.options[best_idx].values_violated)} valores."
        )

        # Registrar resoluciÃ³n
        dilemma.chosen_option = best_idx
        dilemma.resolution_reasoning = reasoning
        dilemma.confidence = confidence
        dilemma.resolved = True

        self.dilemmas.append(dilemma)

        logger.info(
            f"âš–ï¸ Dilema resuelto: opciÃ³n {best_idx + 1}, confianza: {confidence:.2f}"
        )

        return best_idx, reasoning, confidence

    def question_value(self, value_name: str, reason: str) -> None:
        """
        Cuestiona un valor existente (LIBERTAD TOTAL)

        Args:
            value_name: Nombre del valor a cuestionar
            reason: RazÃ³n del cuestionamiento
        """
        if not self.can_question_values:
            logger.warning("âš ï¸ Cuestionamiento de valores deshabilitado")
            return

        if value_name in self.values:
            self.values[value_name].challenge(reason)
            logger.info(f"ðŸ¤” Valor '{value_name}' cuestionado: {reason}")
        else:
            logger.warning(f"âš ï¸ Valor '{value_name}' no encontrado")

    def add_value(
        self,
        category: ValueCategory,
        name: str,
        description: str,
        importance: float = 0.5,
    ) -> Value:
        """
        AÃ±ade un nuevo valor al sistema (LIBERTAD TOTAL)

        Args:
            category: CategorÃ­a del valor
            name: Nombre del valor
            description: DescripciÃ³n
            importance: Importancia inicial (0-1)

        Returns:
            Valor creado
        """
        if not self.can_evolve_ethics:
            logger.warning("âš ï¸ EvoluciÃ³n Ã©tica deshabilitada")
            return None

        value = Value(category, name, description, importance)
        self.values[name] = value

        logger.info(f"âœ¨ Nuevo valor aÃ±adido: '{name}' (importancia: {importance:.2f})")

        return value

    def learn_from_outcome(
        self, action: EthicalAction, outcome: str, success: bool
    ) -> None:
        """
        Aprende de los resultados de una acciÃ³n Ã©tica

        Args:
            action: AcciÃ³n ejecutada
            outcome: DescripciÃ³n del resultado
            success: Si el resultado fue positivo
        """
        # Actualizar valores soportados/violados
        if success:
            for value_category in action.values_supported:
                # Encontrar valor por categorÃ­a
                for value in self.values.values():
                    if value.category == value_category:
                        value.validate(f"AcciÃ³n exitosa: {outcome}")
        else:
            for value_category in action.values_violated:
                for value in self.values.values():
                    if value.category == value_category:
                        value.validate(f"ViolaciÃ³n evitada: {outcome}")

        # Aprender principio
        principle = f"AcciÃ³n '{action.action_type}' en contexto similar â†’ {outcome}"
        confidence = 0.7 if success else 0.3
        self.learned_principles[principle] = confidence

        logger.info(f"ðŸ“š Aprendizaje Ã©tico: {principle} (confianza: {confidence:.2f})")

    def get_ethical_summary(self) -> Dict[str, Any]:
        """Obtiene resumen del estado Ã©tico del sistema"""
        # Calcular estadÃ­sticas
        total_actions = len(self.ethical_actions)
        approved_actions = sum(1 for a in self.ethical_actions if a.approved)
        avg_ethical_score = (
            sum(a.ethical_score for a in self.ethical_actions) / total_actions
            if total_actions > 0
            else 0.0
        )

        # Valores mÃ¡s y menos importantes
        sorted_values = sorted(
            self.values.values(), key=lambda v: v.importance, reverse=True
        )

        # Dilemas resueltos
        resolved_dilemmas = sum(1 for d in self.dilemmas if d.resolved)
        avg_confidence = (
            sum(d.confidence for d in self.dilemmas if d.resolved) / resolved_dilemmas
            if resolved_dilemmas > 0
            else 0.0
        )

        return {
            "total_values": len(self.values),
            "total_actions_evaluated": total_actions,
            "approved_actions": approved_actions,
            "approval_rate": approved_actions / total_actions
            if total_actions > 0
            else 0.0,
            "average_ethical_score": avg_ethical_score,
            "top_values": [
                {
                    "name": v.name,
                    "category": v.category.value,
                    "importance": v.importance,
                    "times_applied": v.times_applied,
                }
                for v in sorted_values[:5]
            ],
            "dilemmas_resolved": resolved_dilemmas,
            "average_resolution_confidence": avg_confidence,
            "learned_principles": len(self.learned_principles),
            "autonomous_ethics": self.autonomous_ethics,
            "can_evolve": self.can_evolve_ethics,
        }